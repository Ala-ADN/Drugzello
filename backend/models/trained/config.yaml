K: 2
batch_size: 32
beta_sparsity: 0.01
delta_decor: 0.05
dropout: 0.1
gamma_exp: 0.1
heads_gat: 1
hidden_channels: 60
layer_norm: true
learning_rate: 0.001
log_interval: 10
model_save_path: models/trained
num_epochs: 20
num_layers: 4
patience: 20
random_seed: 42
residual: true
save_model: true
test_split: 0.1
train_split: 0.8
use_edge_features: false
val_split: 0.1
weight_decay: 1.0e-05
