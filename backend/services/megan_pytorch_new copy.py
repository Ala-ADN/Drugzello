# -*- coding: utf-8 -*-
"""megan_pytorch_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOLIRonsv-wv_5gBr4YXttkbY5iXhMfB
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_add_pool
from torch_geometric.utils import add_self_loops, softmax
from torch_scatter import scatter
from typing import Optional, Tuple, List
from torch import Tensor
import numpy as np

################################################################################
#                              GATv2WithLogits                                 #
################################################################################
class GATv2WithLogits(nn.Module):
    """
    Custom GATv2‐style attention layer that explicitly stores raw (pre‐softmax)
    attention logits in `latest_logits`.
    """
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        edge_dim: int = 0,
        heads: int = 1,
        concat: bool = False,
        use_edge_features: bool = False,
        dropout: float = 0.0,
        add_bias: bool = True,
    ):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.edge_dim = edge_dim
        self.heads = heads
        self.concat = concat
        self.use_edge_features = use_edge_features
        self.dropout = dropout

        # Node feature projection
        self.lin_node = nn.Linear(in_channels, heads * out_channels, bias=False)

        # Edge feature projection (if using edge features)
        if use_edge_features and edge_dim > 0:
            self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False)
            # Attention parameter includes edge features
            self.att = nn.Parameter(torch.Tensor(1, heads, 3 * out_channels))
        else:
            self.lin_edge = None
            # Standard attention parameter (node features only)
            self.att = nn.Parameter(torch.Tensor(1, heads, 2 * out_channels))

        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)
        self.dropout_layer = nn.Dropout(dropout)

        # Bias
        if add_bias:
            if concat:
                self.bias = nn.Parameter(torch.Tensor(heads * out_channels))
            else:
                self.bias = nn.Parameter(torch.Tensor(out_channels))
        else:
            self.register_parameter('bias', None)

        # Storage for attention logits
        self._raw_logits: Optional[Tensor] = None
        self.latest_logits: Optional[Tensor] = None

        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.lin_node.weight)
        if self.lin_edge is not None:
            nn.init.xavier_uniform_(self.lin_edge.weight)
        nn.init.xavier_uniform_(self.att)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(
        self,
        x: Tensor,                          # [N, in_channels]
        edge_index: Tensor,                 # [2, E]
        edge_attr: Optional[Tensor] = None, # [E, edge_dim]
        return_attention: bool = False
    ) -> Tensor:
        """
        Forward pass with optional edge features.
        """
        N = x.size(0)
        E = edge_index.size(1)

        # Project node features
        h = self.lin_node(x)                                    # [N, heads*out_channels]
        h = h.view(N, self.heads, self.out_channels)            # [N, heads, out_channels]

        # Get source and destination node embeddings
        src, dst = edge_index                                   # each is [E]
        h_src = h[src]                                          # [E, heads, out_channels]
        h_dst = h[dst]                                          # [E, heads, out_channels]

        # Compute attention logits
        if self.use_edge_features and edge_attr is not None and self.lin_edge is not None:
            # Project edge features
            h_edge = self.lin_edge(edge_attr)                   # [E, heads*out_channels]
            h_edge = h_edge.view(E, self.heads, self.out_channels)  # [E, heads, out_channels]

            # Concatenate [h_dst || h_src || h_edge]
            h_cat = torch.cat([h_dst, h_src, h_edge], dim=-1)   # [E, heads, 3*out_channels]
        else:
            # Standard concatenation [h_dst || h_src]
            h_cat = torch.cat([h_dst, h_src], dim=-1)           # [E, heads, 2*out_channels]

        # Compute raw attention logits
        e = (h_cat * self.att).sum(dim=-1)                      # [E, heads]
        e = self.leaky_relu(e)                                  # [E, heads]

        # Store raw logits
        self._raw_logits = e.clone()

        # Apply softmax
        alpha = softmax(e, dst, num_nodes=N)                    # [E, heads]
        alpha = self.dropout_layer(alpha)                       # [E, heads]

        # Compute messages and aggregate
        m = h_src * alpha.unsqueeze(-1)                         # [E, heads, out_channels]
        out = scatter(m, dst, dim=0, dim_size=N, reduce='sum')  # [N, heads, out_channels]

        # Handle head concatenation/averaging
        if self.concat:
            out = out.view(N, self.heads * self.out_channels)   # [N, heads * out_channels]
        else:
            out = out.mean(dim=1)                               # [N, out_channels]

        # Add bias
        if self.bias is not None:
            out = out + self.bias

        # Store logits if requested
        if return_attention:
            self.latest_logits = self._raw_logits
        else:
            self.latest_logits = None

        return out

################################################################################
#                                   MEGANCore                                  #
################################################################################
class MEGANCore(nn.Module):
    """
    MEGAN (Multi‐Explanation Graph Attention Network) core architecture.
    Implements L layers, each with K “explanation” heads, uses raw logits for
    E^{im} and V^{im} as in the paper.
    """
    def __init__(
        self,
        in_channels: int,
        hidden_channels: int = 60,
        out_channels: int = 1,
        edge_dim: int = 0,
        num_layers: int = 4,
        K: int = 2,
        heads_gat: int = 1,
        use_edge_features: bool = False,
        add_self_loops: bool = True,
        dropout: float = 0.1,
        layer_norm: bool = True,
        residual: bool = True,
    ):
        super().__init__()
        self.K = K
        self.num_layers = num_layers
        self.add_self_loops = add_self_loops
        self.use_edge_features = use_edge_features
        self.residual = residual

        # Build attention layers
        self.attn_layers = nn.ModuleList()
        for layer_idx in range(self.num_layers):
            curr_in = in_channels if layer_idx == 0 else hidden_channels
            layer_k = nn.ModuleList([
                GATv2WithLogits(
                    in_channels=curr_in,
                    out_channels=hidden_channels,
                    edge_dim=edge_dim,
                    heads=heads_gat,
                    concat=False,
                    use_edge_features=use_edge_features,
                    dropout=dropout,
                )
                for _ in range(K)
            ])
            self.attn_layers.append(layer_k)

        # Layer normalization (optional)
        if layer_norm:
            self.layer_norms = nn.ModuleList([
                nn.LayerNorm(hidden_channels) for _ in range(num_layers)
            ])
        else:
            self.layer_norms = None

        # Residual projection for first layer (if input != hidden dims)
        if residual and in_channels != hidden_channels:
            self.input_proj = nn.Linear(in_channels, hidden_channels)
        else:
            self.input_proj = None

        # Node importance projection units
        self.node_proj_k_units = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_channels // 2, 1),
            ) for _ in range(K)
        ])

        # Enhanced final MLP with better regularization
        self.graph_pred_mlp = nn.Sequential(
            nn.Linear(K * hidden_channels, hidden_channels * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels * 2, hidden_channels),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels, out_channels),
        )

    def forward(
        self,
        x: Tensor,                          # [V_total, in_channels]
        edge_index: Tensor,                 # [2, E_orig]
        batch: Tensor,                      # [V_total]
        edge_attr: Optional[Tensor] = None  # [E_orig, edge_dim]
    ) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Enhanced forward pass with edge features support.
        """
        V_total = x.size(0)
        effective_edge_index = edge_index
        effective_edge_attr = edge_attr

        # Add self-loops
        if self.add_self_loops:
            effective_edge_index, _ = add_self_loops(edge_index, num_nodes=V_total)

            # Handle edge attributes for self-loops
            if edge_attr is not None:
                # Create zero edge attributes for self-loops
                num_self_loops = V_total
                self_loop_attr = torch.zeros(
                    (num_self_loops, edge_attr.size(1)),
                    device=edge_attr.device,
                    dtype=edge_attr.dtype
                )
                effective_edge_attr = torch.cat([edge_attr, self_loop_attr], dim=0)

        E_effective = effective_edge_index.size(1)

        # Initialize node representations
        H_current = x

        # Accumulate attention logits across layers for each explanation channel
        accumulated_A_lk_logits = [
            torch.zeros(E_effective, device=x.device) for _ in range(self.K)
        ]

        # Pass through layers
        for l_idx in range(self.num_layers):
            H_next_list = []

            for k_idx in range(self.K):
                gat_layer = self.attn_layers[l_idx][k_idx]

                # Forward through GAT layer
                H_lk = gat_layer(
                    H_current,
                    effective_edge_index,
                    edge_attr=effective_edge_attr,
                    return_attention=True
                )

                # Apply activation
                H_lk = F.elu(H_lk)
                H_next_list.append(H_lk)

                # Accumulate attention logits
                A_lk_raw = gat_layer.latest_logits.mean(dim=1)  # Average over heads
                accumulated_A_lk_logits[k_idx] += A_lk_raw

            # Average across explanation channels
            H_next = torch.stack(H_next_list, dim=0).mean(dim=0)

            # Residual connection
            if self.residual:
                if l_idx == 0 and self.input_proj is not None:
                    H_current = H_next + self.input_proj(H_current)
                elif l_idx > 0:
                    H_current = H_next + H_current
                else:
                    H_current = H_next
            else:
                H_current = H_next

            # Layer normalization
            if self.layer_norms is not None:
                H_current = self.layer_norms[l_idx](H_current)

        H_L = H_current

        # Compute edge importance (Eim)
        Eim_logits = torch.stack(accumulated_A_lk_logits, dim=1)
        Eim = torch.sigmoid(Eim_logits)

        # Compute node importance (Vim)
        Vim_channels = []
        for k_idx in range(self.K):
            # Pool edge importance to nodes
            E_p_k = torch.zeros(V_total, device=x.device)
            E_p_k.index_add_(0, effective_edge_index[1], Eim[:, k_idx])

            # Compute learned node importance
            Vtilde_logits = self.node_proj_k_units[k_idx](H_L)
            Vtilde = torch.sigmoid(Vtilde_logits).squeeze(-1)

            # Combine edge-pooled and learned importance
            Vim_k = Vtilde * E_p_k
            Vim_channels.append(Vim_k.unsqueeze(1))

        Vim = torch.cat(Vim_channels, dim=1)

        # Graph-level embedding computation
        graph_embeddings = []
        for k_idx in range(self.K):
            weighted_H_k = H_L * Vim[:, k_idx].unsqueeze(1)
            h_k = global_add_pool(weighted_H_k, batch)
            graph_embeddings.append(h_k)

        h_cat = torch.cat(graph_embeddings, dim=1)

        # Final prediction
        out_y = self.graph_pred_mlp(h_cat)

        return out_y, Eim, Vim

# Example dimensions
F_in_channels = 16
H_hidden_channels = 8
C_out_channels = 2
L_num_layers = 2
K_explanation_channels = 2
GAT_internal_heads = 3
ADD_SELF_LOOPS_FLAG = True

# Suppose we have a batch of 2 graphs:
num_nodes_graph1 = 5
num_edges_graph1 = 8
num_nodes_graph2 = 7
num_edges_graph2 = 10
B_batch_size = 2

V_total_nodes = num_nodes_graph1 + num_nodes_graph2
E_original_edges = num_edges_graph1 + num_edges_graph2

# If we add self‐loops, we get one extra edge per node
E_effective_edges = E_original_edges + (V_total_nodes if ADD_SELF_LOOPS_FLAG else 0)

# Create dummy node features
x_nodes = torch.randn(V_total_nodes, F_in_channels)

# Create dummy edge_index for Graph 1
edge_index1_src = torch.randint(0, num_nodes_graph1, (num_edges_graph1,))
edge_index1_dst = torch.randint(0, num_nodes_graph1, (num_edges_graph1,))
edge_index1 = torch.stack([edge_index1_src, edge_index1_dst], dim=0)

# Create dummy edge_index for Graph 2 (with indices offset by num_nodes_graph1)
edge_index2_src = torch.randint(0, num_nodes_graph2, (num_edges_graph2,)) + num_nodes_graph1
edge_index2_dst = torch.randint(0, num_nodes_graph2, (num_edges_graph2,)) + num_nodes_graph1
edge_index2 = torch.stack([edge_index2_src, edge_index2_dst], dim=0)

# Concatenate to form a single batch edge_index
edge_index_graph_original = torch.cat([edge_index1, edge_index2], dim=1).long()

# Batch vector to indicate which graph each node belongs to
batch_vector = torch.cat([
    torch.zeros(num_nodes_graph1, dtype=torch.long),
    torch.ones(num_nodes_graph2, dtype=torch.long),
])  # [V_total_nodes]

# Instantiate MEGANCore with our corrected GATv2WithLogits
megan_model = MEGANCore(
    in_channels=F_in_channels,
    hidden_channels=H_hidden_channels,
    out_channels=C_out_channels,
    num_layers=L_num_layers,
    K=K_explanation_channels,
    heads_gat=GAT_internal_heads,
    add_self_loops=ADD_SELF_LOOPS_FLAG,
)
megan_model.eval()

# Forward pass
y_pred, Eim_expl, Vim_expl = megan_model(x_nodes, edge_index_graph_original, batch_vector)

# Verify shapes
print("\n--- Shapes & Checks ---")
print(f"x_nodes           : {x_nodes.shape}")                                # [V_total_nodes, 16]
print(f"edge_index        : {edge_index_graph_original.shape}")              # [2, E_orig]
print(f"batch_vector      : {batch_vector.shape}")                           # [V_total_nodes]

print(f"V_total_nodes     : {V_total_nodes}")
print(f"E_original_edges  : {E_original_edges}")
print(f"E_effective_edges : {E_effective_edges}")
print(f"K_explanation     : {K_explanation_channels}")
print(f"B_batch_size      : {B_batch_size}")
print(f"C_out_channels    : {C_out_channels}")

print(f"y_pred shape      : {y_pred.shape}   (Expected: [{B_batch_size}, {C_out_channels}])")
print(f"Eim_expl shape    : {Eim_expl.shape} (Expected: [{E_effective_edges}, {K_explanation_channels}])")
print(f"Vim_expl shape    : {Vim_expl.shape} (Expected: [{V_total_nodes}, {K_explanation_channels}])")

assert y_pred.shape == (B_batch_size, C_out_channels)
assert Eim_expl.shape == (E_effective_edges, K_explanation_channels)
assert Vim_expl.shape == (V_total_nodes, K_explanation_channels)

print("\nAll output shapes match the MEGAN architecture expectations.")

################################################################################
#       Training Configuration (with the best result from smart search)        #
################################################################################

class MEGANConfig:
    """Configuration class for MEGAN hyperparameters with presets - ENHANCED VERSION."""

    def __init__(self, preset: str = "default"):
        if preset == "default":
            self.setup_default()
        elif preset == "large":
            self.setup_large()
        elif preset == "small":
            self.setup_small()
        elif preset == "edge_focused":
            self.setup_edge_focused()
        elif preset == "loss_balanced":
            self.setup_loss_balanced()
        else:
            raise ValueError(f"Unknown preset: {preset}")

    def setup_default(self):
        """Default configuration similar to original implementation."""
        self.hidden_channels = 60
        self.num_layers = 4
        self.K = 2
        self.heads_gat = 1
        self.use_edge_features = True
        self.dropout = 0.1
        self.layer_norm = True
        self.residual = True
        self.learning_rate = 5e-4
        self.weight_decay = 1e-5
        self.batch_size = 32
        self.epochs = 150
        # Loss weights
        self.gamma_exp = 0.1
        self.beta_sparsity = 0.01
        self.delta_decor = 0.05

    def setup_large(self):
        """Larger model for complex datasets."""
        self.hidden_channels = 128
        self.num_layers = 6
        self.K = 4
        self.heads_gat = 2
        self.use_edge_features = True
        self.dropout = 0.15
        self.layer_norm = True
        self.residual = True
        self.learning_rate = 3e-4
        self.weight_decay = 1e-4
        self.batch_size = 16
        self.epochs = 200
        # Loss weights for larger model
        self.gamma_exp = 0.05
        self.beta_sparsity = 0.02
        self.delta_decor = 0.1

    def setup_small(self):
        """Smaller model for quick experiments."""
        self.hidden_channels = 32
        self.num_layers = 3
        self.K = 2
        self.heads_gat = 1
        self.use_edge_features = False
        self.dropout = 0.05
        self.layer_norm = False
        self.residual = False
        self.learning_rate = 1e-3
        self.weight_decay = 1e-6
        self.batch_size = 64
        self.epochs = 100
        # Loss weights for small model
        self.gamma_exp = 0.2
        self.beta_sparsity = 0.005
        self.delta_decor = 0.02

    def setup_edge_focused(self):
        """Configuration optimized for edge feature utilization."""
        self.hidden_channels = 128
        self.num_layers = 3
        self.K = 2
        self.heads_gat = 4
        self.use_edge_features = True
        self.dropout = 0.1
        self.layer_norm = False
        self.residual = True
        self.learning_rate = 0.001
        self.weight_decay = 1e-06
        self.batch_size = 16
        self.epochs = 175
        # Loss weights for edge-focused model
        self.gamma_exp = 0.1
        self.beta_sparsity = 0.01
        self.delta_decor = 0.05

    def setup_loss_balanced(self):
        """Configuration focused on balanced loss components."""
        self.hidden_channels = 80
        self.num_layers = 4
        self.K = 3
        self.heads_gat = 2
        self.use_edge_features = True
        self.dropout = 0.12
        self.layer_norm = True
        self.residual = True
        self.learning_rate = 4e-4
        self.weight_decay = 5e-5
        self.batch_size = 24
        self.epochs = 160
        # Carefully balanced loss weights
        self.gamma_exp = 0.08
        self.beta_sparsity = 0.012
        self.delta_decor = 0.06

################################################################################
#                              Learning Rate Scheduler                         #
################################################################################
class WarmupCosineScheduler:
    """Custom learning rate scheduler with warmup and cosine annealing."""

    def __init__(self, optimizer, warmup_epochs: int, total_epochs: int,
                 base_lr: float, min_lr: float = 1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_epoch = 0

    def step(self):
        if self.current_epoch < self.warmup_epochs:
            # Warmup phase
            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs
        else:
            # Cosine annealing phase
            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr

        self.current_epoch += 1
        return lr

################################################################################
#                       Fidelity* Computation for Explanations                 #
################################################################################

def compute_fidelity_star(model, x, edge_index, batch, edge_attr, k_mask_index, Vim, K):
    """
    Computes Fidelity* by masking the k-th explanation channel.

    Args:
        model: Trained MEGAN model
        x: Node features [V, F]
        edge_index: Edge connectivity [2, E]
        batch: Batch indices [V]
        edge_attr: Edge attributes [E, edge_dim] or None
        k_mask_index: Channel index to mask (0 to K-1)
        Vim: Node importance scores [V, K]
        K: Number of explanation channels

    Returns:
        y_masked: Predictions with masked channel
    """
    with torch.no_grad():
        # Get original prediction and node embeddings
        y_orig, _, _ = model(x, edge_index, batch, edge_attr)

        # Create a copy of the model for modified forward pass
        model_copy = model

        # Create mask that zeros out the k-th channel
        M_mask = torch.ones_like(Vim)
        M_mask[:, k_mask_index] = 0.0

        # Masked Vim
        Vim_masked = Vim * M_mask

        # Get node embeddings (reuse model's final layer representations)
        H_current = x
        for l_idx in range(model.num_layers):
            H_next_list = []
            for k_idx in range(K):
                gat_layer = model.attn_layers[l_idx][k_idx]
                H_lk = gat_layer(H_current, edge_index, edge_attr, return_attention=False)
                H_lk = F.elu(H_lk)
                H_next_list.append(H_lk)

            H_next = torch.stack(H_next_list, dim=0).mean(dim=0)

            if model.residual:
                if l_idx == 0 and model.input_proj is not None:
                    H_current = H_next + model.input_proj(H_current)
                elif l_idx > 0:
                    H_current = H_next + H_current
                else:
                    H_current = H_next
            else:
                H_current = H_next

            if model.layer_norms is not None:
                H_current = model.layer_norms[l_idx](H_current)

        H_L = H_current

        # Graph-level embedding computation with masked importance
        graph_embeddings = []
        for k_idx in range(K):
            weighted_H_k = H_L * Vim_masked[:, k_idx].unsqueeze(1)
            h_k = global_add_pool(weighted_H_k, batch)
            graph_embeddings.append(h_k)

        h_cat_masked = torch.cat(graph_embeddings, dim=1)
        y_masked = model.graph_pred_mlp(h_cat_masked)

        return y_masked, y_orig

def evaluate_explanation_fidelity(model, dataset, test_indices, config, device):
    """
    Evaluate explanation fidelity across test set.
    """
    from torch.utils.data import Subset
    from torch_geometric.loader import DataLoader

    test_subset = Subset(dataset, test_indices)
    test_loader = DataLoader(test_subset, batch_size=1, shuffle=False)

    fidelity_scores = {f'channel_{k}': [] for k in range(config.K)}

    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            x = batch.x.float()
            edge_index = batch.edge_index
            batch_mask = batch.batch

            edge_attr = None
            if config.use_edge_features and hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                edge_attr = batch.edge_attr.float()

            # Get original prediction and importance scores
            y_orig, _, Vim = model(x, edge_index, batch_mask, edge_attr)

            # Compute fidelity for each channel
            for k in range(config.K):
                y_masked, _ = compute_fidelity_star(
                    model, x, edge_index, batch_mask, edge_attr, k, Vim, config.K
                )

                # Fidelity* = |y_orig - y_masked|
                fidelity = torch.abs(y_orig - y_masked).item()
                fidelity_scores[f'channel_{k}'].append(fidelity)

    # Compute statistics
    fidelity_stats = {}
    for channel, scores in fidelity_scores.items():
        fidelity_stats[channel] = {
            'mean': np.mean(scores),
            'std': np.std(scores),
            'median': np.median(scores)
        }

    return fidelity_stats

def channel_decorrelation_loss(Vim):
    """
    Computes decorrelation loss to ensure channels capture different information.

    Args:
        Vim: Node importance scores [V, K]

    Returns:
        Decorrelation loss
    """
    if Vim.shape[1] <= 1:  # Only one channel
        return torch.tensor(0.0, device=Vim.device)

    # Normalize across nodes (L2 norm)
    Vim_norm = F.normalize(Vim, p=2, dim=0)  # [V, K]

    # Compute gram matrix (channel correlations)
    gram_matrix = torch.mm(Vim_norm.T, Vim_norm)  # [K, K]

    # Identity matrix
    identity = torch.eye(Vim.shape[1], device=Vim.device)

    # Loss: minimize off-diagonal elements
    loss = ((gram_matrix - identity)**2).sum()

    return loss

def sparsity_loss(Vim, Eim):
    """
    Computes sparsity loss to encourage sparse explanations.

    Args:
        Vim: Node importance scores [V, K]
        Eim: Edge importance scores [E, K]

    Returns:
        Sparsity loss
    """
    node_sparsity = torch.norm(Vim, p=1) / Vim.numel()
    edge_sparsity = torch.norm(Eim, p=1) / Eim.numel()

    return node_sparsity + edge_sparsity

def explanation_fidelity_loss(model, x, edge_index, batch, edge_attr, Vim, Eim, y_true, channel_idx):
    """
    Mask out one explanation channel and see how prediction changes.

    Args:
        model: Trained MEGAN model
        x: Node features [V, F]
        edge_index: Edge connectivity [2, E]
        batch: Batch indices [V]
        edge_attr: Edge attributes [E, edge_dim] or None
        Vim: Node importance scores [V, K]
        Eim: Edge importance scores [E, K]
        y_true: True labels for the batch
        channel_idx: Index of the channel to mask (0 to K-1)

    Returns:
        fidelity_loss: Loss indicating how much the prediction changed
    """
    # Zero out Vim for the selected channel
    Vim_masked = Vim.clone()
    Vim_masked[:, channel_idx] = 0.0

    # Same for Eim if used in edge weighting
    # Eim_masked = Eim.clone(); Eim_masked[:, channel_idx] = 0.0  # Optional

    # Multiply original H_L with masked Vim
    weighted_x = x * Vim_masked[:, channel_idx].unsqueeze(1)

    # Re-run forward pass (simplified here)
    y_masked, _, _ = model(weighted_x, edge_index, batch, edge_attr)

    # Loss: how much prediction changed
    fidelity_loss = F.mse_loss(y_masked, y_true)

    return fidelity_loss

####################################################################################
#                    Training Function with Multiple Loss Components               #
####################################################################################

def train_megan(
    dataset,
    config: MEGANConfig,
    fold_splits: List[Tuple],
    device: torch.device,
    verbose: bool = True,
    save_models: bool = True,
    save_dir: str = "saved_models",
):
    """
    Training function with model saving capability.
    """
    from torch.utils.data import Subset
    from torch_geometric.loader import DataLoader
    import os
    import pickle

    if save_models:
        os.makedirs(save_dir, exist_ok=True)

    fold_results = []
    overall_best_val_loss = float("inf")
    overall_best_model_state = None
    overall_best_fold_idx = None

    # Extract loss weights from config - NEW
    gamma_exp = getattr(config, 'gamma_exp', 0.1)
    beta_sparsity = getattr(config, 'beta_sparsity', 0.01)
    delta_decor = getattr(config, 'delta_decor', 0.05)

    for fold_idx, (train_idx, test_idx) in enumerate(fold_splits):
        if verbose:
            print(f"\n===== Fold {fold_idx + 1} / {len(fold_splits)} =====")

        # Create data loaders
        train_subset = Subset(dataset, train_idx)
        test_subset = Subset(dataset, test_idx)
        train_loader = DataLoader(train_subset, batch_size=config.batch_size, shuffle=True)
        test_loader = DataLoader(test_subset, batch_size=config.batch_size, shuffle=False)

        # Initialize model
        model_kwargs = {
            "in_channels": dataset.num_node_features,
            "hidden_channels": config.hidden_channels,
            "out_channels": 1,
            "edge_dim": getattr(dataset, 'num_edge_features', 0),
            "num_layers": config.num_layers,
            "K": config.K,
            "heads_gat": config.heads_gat,
            "use_edge_features": config.use_edge_features,
            "dropout": config.dropout,
            "layer_norm": config.layer_norm,
            "residual": config.residual,
        }

        model = MEGANCore(**model_kwargs).to(device)

        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        criterion = nn.L1Loss()

        # Learning rate scheduler
        scheduler = WarmupCosineScheduler(
            optimizer,
            warmup_epochs=config.epochs // 10,
            total_epochs=config.epochs,
            base_lr=config.learning_rate
        )

        best_val_loss = float("inf")
        best_model_state = None

        # Loss tracking
        losses = {
            "total": [], "pred": [], "exp": [], "sparsity": [], "decor": [], "val": []
        }

        for epoch in range(config.epochs):
            # Training
            model.train()
            train_loss_accum = 0.0
            pred_loss_accum = 0.0
            exp_loss_accum = 0.0
            sparsity_loss_accum = 0.0
            decor_loss_accum = 0.0
            train_count = 0

            for batch in train_loader:
                batch = batch.to(device)
                x = batch.x.float()
                edge_index = batch.edge_index
                batch_mask = batch.batch
                y_true = batch.y.view(-1, 1)

                edge_attr = None
                if config.use_edge_features and hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                    edge_attr = batch.edge_attr.float()

                optimizer.zero_grad()
                y_pred, Eim, Vim = model(x, edge_index, batch_mask, edge_attr)

                # Primary prediction loss
                loss_pred = criterion(y_pred, y_true)

                # Regularization losses
                loss_sparsity = sparsity_loss(Vim, Eim)
                loss_decor = channel_decorrelation_loss(Vim)
                loss_explanation = explanation_fidelity_loss(
                    model, x, edge_index, batch_mask, edge_attr, Vim, Eim, y_true, channel_idx=0
                )

                # Total loss
                total_loss = (loss_pred +
                            gamma_exp * loss_explanation +
                            beta_sparsity * loss_sparsity +
                            delta_decor * loss_decor)

                total_loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                # Accumulate losses
                batch_size = y_pred.size(0)
                train_loss_accum += total_loss.item() * batch_size
                pred_loss_accum += loss_pred.item() * batch_size
                exp_loss_accum += loss_explanation.item() * batch_size
                sparsity_loss_accum += loss_sparsity.item() * batch_size
                decor_loss_accum += loss_decor.item() * batch_size
                train_count += batch_size

            # Update learning rate
            current_lr = scheduler.step()

            # Average losses
            avg_total_loss = train_loss_accum / train_count
            avg_pred_loss = pred_loss_accum / train_count
            avg_exp_loss = exp_loss_accum / train_count
            avg_sparsity_loss = sparsity_loss_accum / train_count
            avg_decor_loss = decor_loss_accum / train_count

            # Store losses
            losses["total"].append(avg_total_loss)
            losses["pred"].append(avg_pred_loss)
            losses["exp"].append(avg_exp_loss)
            losses["sparsity"].append(avg_sparsity_loss)
            losses["decor"].append(avg_decor_loss)

            # Validation
            model.eval()
            val_loss_accum = 0.0
            val_count = 0

            with torch.no_grad():
                for batch in test_loader:
                    batch = batch.to(device)
                    x = batch.x.float()
                    edge_index = batch.edge_index
                    batch_mask = batch.batch
                    y_true = batch.y.view(-1, 1)

                    edge_attr = None
                    if config.use_edge_features and hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                        edge_attr = batch.edge_attr.float()

                    y_pred, _, _ = model(x, edge_index, batch_mask, edge_attr)
                    loss_val = criterion(y_pred, y_true)
                    val_loss_accum += loss_val.item() * y_pred.size(0)
                    val_count += y_pred.size(0)

            avg_val_loss = val_loss_accum / val_count
            losses["val"].append(avg_val_loss)

            # Save best model state for this fold
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = {
                    'epoch': epoch,
                    'fold_idx': fold_idx,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state': scheduler.current_epoch,
                    'val_loss': best_val_loss,
                    'train_loss': avg_total_loss,
                    'config': config,
                    'model_kwargs': model_kwargs,
                    'losses': losses
                }

            # Logging
            if verbose and ((epoch + 1) % 25 == 0 or epoch == 0):
                print(f"  Epoch {epoch+1:03d} | LR: {current_lr:.2e} | "
                      f"Total: {avg_total_loss:.5f}  | Pred: {avg_pred_loss:.5f} | "
                      f"Exp: {avg_exp_loss} | Sparse: {avg_sparsity_loss:.5f} | "
                      f"Decor: {avg_decor_loss:.5f} | Val: {avg_val_loss:.5f}")

        if verbose:
            print(f"→ Fold {fold_idx} best Val MAE: {best_val_loss:.5f}")

        # Check if this fold has the best overall performance
        if best_val_loss < overall_best_val_loss:
            overall_best_val_loss = best_val_loss
            overall_best_model_state = best_model_state.copy()  # Make a copy
            overall_best_fold_idx = fold_idx
            if verbose:
                print(f"  → New overall best model found in fold {fold_idx}!")

        # Evaluate explanation fidelity
        if best_model_state is not None:
            model.load_state_dict(best_model_state['model_state_dict'])
            fidelity_stats = evaluate_explanation_fidelity(
                model, dataset, test_idx, config, device
            )
        else:
            fidelity_stats = {}

        # Extract final importances per graph (same as before)
        from torch_geometric.loader import DataLoader as SingleLoader
        single_loader = SingleLoader(test_subset, batch_size=1, shuffle=False)

        graph_Eim, graph_Vim = [], []
        model.eval()
        with torch.no_grad():
            for single in single_loader:
                single = single.to(device)
                x = single.x.float()
                edge_index = single.edge_index
                batch_mask = single.batch
                edge_attr = None
                if config.use_edge_features and hasattr(single, 'edge_attr') and single.edge_attr is not None:
                    edge_attr = single.edge_attr.float()

                _, Eim_single, Vim_single = model(x, edge_index, batch_mask, edge_attr)

                E_effective = Eim_single.size(0)
                E_orig = single.edge_index.size(1)
                Eim_full = Eim_single.cpu().numpy()
                Eim_orig = Eim_full[:E_orig, :]
                graph_Eim.append(Eim_orig)

                Vim_arr = Vim_single.cpu().numpy()
                graph_Vim.append(Vim_arr)

        fold_result = {
            'fold_idx': fold_idx,
            'best_val_mae': best_val_loss,
            'final_train_mae': avg_pred_loss,
            'losses': losses,
            'fidelity_stats': fidelity_stats,
            'Eim': graph_Eim,
            'Vim': graph_Vim,
            'config': config,
            'test_indices': test_idx
        }

        fold_results.append(fold_result)

    # Save best model and results
    if save_models and overall_best_model_state is not None:
        model_filename = "megan_pytorch_model.pth"
        model_path = os.path.join(save_dir, model_filename)
        torch.save(overall_best_model_state, model_path)

        # Save training summary
        training_summary = {
            'fold_results': fold_results,
            'overall_best_fold': overall_best_fold_idx,
            'overall_best_val_mae': overall_best_val_loss,  # This is the actual best, not average
            'best_model_path': model_path,
            'config': config
        }

        results_filename = "training_results.pkl"
        results_path = os.path.join(save_dir, results_filename)
        with open(results_path, 'wb') as f:
            pickle.dump(training_summary, f)

        if verbose:
            print(f"\n→ Overall best model saved to: {model_path}")
            print(f"→ Overall best validation MAE: {overall_best_val_loss:.5f} (from fold {overall_best_fold_idx})")

            # Also show the average for comparison
            avg_val_mae = np.mean([r['best_val_mae'] for r in fold_results])
            print(f"→ Average validation MAE across folds: {avg_val_mae:.5f}")

    return fold_results

####################################################################################
#          Utility functions for Loading and Predicting with Saved Model           #
####################################################################################


def load_megan_model(model_path: str, device: torch.device):
    """
    Load a saved MEGAN model.

    Args:
        model_path: Path to the saved model file
        device: Device to load the model on

    Returns:
        Tuple of (model, config, training_info)
    """
    checkpoint = torch.load(model_path, map_location=device)

    # Recreate model
    model = MEGANCore(**checkpoint['model_kwargs']).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model, checkpoint['config'], checkpoint

def get_model_summary(save_dir: str):
    """
    Get a summary of the best saved model and all fold performances.

    Args:
        save_dir: Directory containing saved models

    Returns:
        Dictionary with model summary information
    """
    import pickle
    import os

    results_path = os.path.join(save_dir, "training_results.pkl")
    if not os.path.exists(results_path):
        return {"error": "No training results found"}

    with open(results_path, 'rb') as f:
        training_summary = pickle.load(f)

    fold_results = training_summary['fold_results']

    summary = {
        'num_folds': len(fold_results),
        'overall_best_fold': training_summary.get('overall_best_fold'),
        'overall_best_val_mae': training_summary.get('overall_best_val_mae'),
        'best_model_path': training_summary.get('best_model_path'),
        'fold_performances': []
    }

    for fold_result in fold_results:
        fold_info = {
            'fold_idx': fold_result['fold_idx'],
            'best_val_mae': fold_result['best_val_mae'],
            'final_train_mae': fold_result['final_train_mae']
        }
        summary['fold_performances'].append(fold_info)

    # Calculate statistics
    val_maes = [f['best_val_mae'] for f in fold_results]
    summary['mean_val_mae'] = np.mean(val_maes)
    summary['std_val_mae'] = np.std(val_maes)
    summary['min_val_mae'] = np.min(val_maes)
    summary['max_val_mae'] = np.max(val_maes)

    return summary

def predict_with_saved_model(model_path: str, dataset, indices: List[int], device: torch.device):
    """
    Make predictions using a saved model.

    Args:
        model_path: Path to saved model
        dataset: PyG dataset
        indices: List of dataset indices to predict
        device: Device for computation

    Returns:
        Tuple of (predictions, node_importances, edge_importances)
    """
    from torch.utils.data import Subset
    from torch_geometric.loader import DataLoader

    # Load model
    model, config, _ = load_megan_model(model_path, device)

    # Create data loader
    subset = Subset(dataset, indices)
    loader = DataLoader(subset, batch_size=1, shuffle=False)

    predictions = []
    node_importances = []
    edge_importances = []

    model.eval()
    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            x = batch.x.float()
            edge_index = batch.edge_index
            batch_mask = batch.batch

            edge_attr = None
            if config.use_edge_features and hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                edge_attr = batch.edge_attr.float()

            y_pred, Eim, Vim = model(x, edge_index, batch_mask, edge_attr)

            predictions.append(y_pred.cpu().numpy())

            # For single molecule predictions, extract original edges
            E_orig = batch.edge_index.size(1)
            Eim_orig = Eim[:E_orig, :].cpu().numpy()
            edge_importances.append(Eim_orig)

            node_importances.append(Vim.cpu().numpy())

    return predictions, node_importances, edge_importances

from torch_geometric.datasets import MoleculeNet
from torch_geometric.transforms import ToUndirected

# =============================================================================
# 1) Load ESOL via PyG’s MoleculeNet
# =============================================================================
#
#   The MoleculeNet class will:
#     - Read the SMILES strings
#     - Use RDKit internally to compute:
#         • atom‐level features (data.x, shape [num_nodes, num_node_features])
#         • adjacency (data.edge_index, shape [2, num_edges])
#         • graph label data.y (shape [1], float32)
#
#   We apply ToUndirected() so that edge_index is made bidirectional explicitly.
#   (MoleculeNet sometimes gives directed adjacency; this ensures undirected graphs.)

dataset = MoleculeNet(root="data/ESOL", name="ESOL", transform=ToUndirected())
print(f"Total molecules in ESOL dataset: {len(dataset)}")
print(f"Node‐feature dimension: {dataset.num_node_features}")
print(f"Edge‐feature dimension: {dataset.num_edge_features if hasattr(dataset, 'num_edge_features') else 'N/A'}")
num_tasks = dataset[0].y.shape[-1] if hasattr(dataset[0], 'y') else 0
print(f"Number of tasks (labels): {num_tasks}")  # Should be 1 for ESOL

# Each `data` in `dataset` has:
#   data.x           → [num_nodes, num_node_features]
#   data.edge_index  → [2, num_edges]
#   data.y           → [1] (solubility in log mol/L)
#   data.batch       → added by DataLoader, not present yet
#   data.idx         → dataset index (optional)

import torch
import numpy as np
from itertools import product
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import time
import random
import pickle
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold

################################################################################
#                           Hyperparameter Search Framework                   #
################################################################################

@dataclass
class SearchSpace:
    """Define hyperparameter search spaces for MEGAN - ENHANCED VERSION."""

    # Architecture parameters
    hidden_channels: List[int] = None
    num_layers: List[int] = None
    K: List[int] = None
    heads_gat: List[int] = None

    # Training parameters
    learning_rate: List[float] = None
    weight_decay: List[float] = None
    dropout: List[float] = None
    batch_size: List[int] = None

    # Feature parameters
    use_edge_features: List[bool] = None
    layer_norm: List[bool] = None
    residual: List[bool] = None

    # Loss weight parameters
    gamma_exp: List[float] = None
    beta_sparsity: List[float] = None
    delta_decor: List[float] = None

    def __post_init__(self):
        """Set default search spaces if not provided."""
        if self.hidden_channels is None:
            self.hidden_channels = [32, 60, 80, 128]
        if self.num_layers is None:
            self.num_layers = [3, 4, 5, 6]
        if self.K is None:
            self.K = [2]
        if self.heads_gat is None:
            self.heads_gat = [1, 2, 4]
        if self.learning_rate is None:
            self.learning_rate = [1e-4, 3e-4, 5e-4, 1e-3]
        if self.weight_decay is None:
            self.weight_decay = [1e-6, 1e-5, 1e-4]
        if self.dropout is None:
            self.dropout = [0.0, 0.05, 0.1, 0.15, 0.2]
        if self.batch_size is None:
            self.batch_size = [16, 32, 64]
        if self.use_edge_features is None:
            self.use_edge_features = [True, False]
        if self.layer_norm is None:
            self.layer_norm = [True, False]
        if self.residual is None:
            self.residual = [True, False]
        # Loss weight search spaces
        if self.gamma_exp is None:
            self.gamma_exp = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25]
        if self.beta_sparsity is None:
            self.beta_sparsity = [0.005, 0.01, 0.015, 0.02, 0.03]
        if self.delta_decor is None:
            self.delta_decor = [0.02, 0.05, 0.08, 0.1, 0.15]

class HyperparameterSearcher:
    """Advanced hyperparameter search with different strategies."""

    def __init__(self, search_space: SearchSpace, strategy: str = "random"):
        self.search_space = search_space
        self.strategy = strategy
        self.results = []
        self.best_config = None
        self.best_score = float('inf')

    def generate_configs(self, n_trials: int = 50) -> List[Dict]:
        """Generate hyperparameter configurations based on strategy."""
        if self.strategy == "grid":
            return self._grid_search()
        elif self.strategy == "random":
            return self._random_search(n_trials)
        elif self.strategy == "smart":
            return self._smart_search(n_trials)
        elif self.strategy == "bayesian":
            return self._bayesian_search(n_trials)
        else:
            raise ValueError(f"Unknown strategy: {self.strategy}")

    def _grid_search(self) -> List[Dict]:
        """Exhaustive grid search (can be very large!)"""
        param_names = []
        param_values = []

        for field, value in asdict(self.search_space).items():
            if isinstance(value, list) and len(value) > 0:
                param_names.append(field)
                param_values.append(value)

        configs = []
        for combination in product(*param_values):
            config = dict(zip(param_names, combination))
            configs.append(config)

        print(f"Grid search will test {len(configs)} configurations")
        return configs

    def _random_search(self, n_trials: int) -> List[Dict]:
        """Random sampling from search space."""
        configs = []
        search_dict = asdict(self.search_space)

        for _ in range(n_trials):
            config = {}
            for param, values in search_dict.items():
                if isinstance(values, list) and len(values) > 0:
                    config[param] = random.choice(values)
            configs.append(config)

        return configs

    def _smart_search(self, n_trials: int) -> List[Dict]:
        """Smart search combining good defaults with random exploration."""
        configs = []

        # Start with some good baseline configurations
        good_configs = [
            # Small, fast model
            {"hidden_channels": 32, "num_layers": 3, "K": 2, "heads_gat": 1,
             "learning_rate": 1e-3, "weight_decay": 1e-5, "dropout": 0.1,
             "batch_size": 64, "use_edge_features": True, "layer_norm": True, "residual": True},

            # Balanced model
            {"hidden_channels": 60, "num_layers": 4, "K": 2, "heads_gat": 1,
             "learning_rate": 5e-4, "weight_decay": 1e-5, "dropout": 0.1,
             "batch_size": 32, "use_edge_features": True, "layer_norm": True, "residual": True},

            # Large model
            {"hidden_channels": 128, "num_layers": 5, "K": 3, "heads_gat": 2,
             "learning_rate": 3e-4, "weight_decay": 1e-4, "dropout": 0.15,
             "batch_size": 16, "use_edge_features": True, "layer_norm": True, "residual": True},

            # Edge-focused model
            {"hidden_channels": 80, "num_layers": 4, "K": 3, "heads_gat": 2,
             "learning_rate": 4e-4, "weight_decay": 5e-5, "dropout": 0.12,
             "batch_size": 24, "use_edge_features": True, "layer_norm": True, "residual": True},
        ]

        # Add baseline configs
        configs.extend(good_configs[:min(len(good_configs), n_trials // 4)])

        # Fill remaining with random search
        remaining_trials = n_trials - len(configs)
        if remaining_trials > 0:
            configs.extend(self._random_search(remaining_trials))

        return configs

    def _bayesian_search(self, n_trials: int) -> List[Dict]:
        """Simplified Bayesian optimization using Gaussian Process."""
        try:
            from sklearn.gaussian_process import GaussianProcessRegressor
            from sklearn.gaussian_process.kernels import Matern
        except ImportError:
            print("scikit-learn not available, falling back to random search")
            return self._random_search(n_trials)

        # Start with a few random configurations
        initial_configs = self._random_search(min(10, n_trials // 3))
        configs = initial_configs.copy()

        # For remaining trials, use acquisition function
        # (This is a simplified version; full implementation would require more sophisticated handling)
        remaining_trials = n_trials - len(configs)
        if remaining_trials > 0:
            configs.extend(self._random_search(remaining_trials))

        return configs

class SearchResultAnalyzer:
    """Analyze and visualize hyperparameter search results."""

    def __init__(self, results: List[Dict]):
        self.results = results
        self.df = self._results_to_dataframe()

    def _results_to_dataframe(self):
        """Convert results to pandas DataFrame for analysis."""
        try:
            import pandas as pd
        except ImportError:
            print("pandas not available for analysis")
            return None

        flattened_results = []
        for result in self.results:
            flat_result = {}
            # Add configuration parameters
            for key, value in result.get('config', {}).items():
                flat_result[f'config_{key}'] = value

            # Add performance metrics
            flat_result['val_mae'] = result.get('val_mae', float('inf'))
            flat_result['train_mae'] = result.get('train_mae', float('inf'))
            flat_result['training_time'] = result.get('training_time', 0)
            flat_result['trial_id'] = result.get('trial_id', -1)

            flattened_results.append(flat_result)

        return pd.DataFrame(flattened_results)

    def get_top_configs(self, n: int = 10, metric: str = 'val_mae'):
        """Get top N configurations based on specified metric."""
        if self.df is None or self.df.empty:
            return []

        # Filter out non-finite metric values before sorting
        df_finite_metric = self.df[np.isfinite(self.df[metric])].copy()
        if df_finite_metric.empty:
            return []

        sorted_df = df_finite_metric.sort_values(metric, ascending=True)
        return sorted_df.head(n).to_dict('records')

    def analyze_parameter_importance(self, metric: str = 'val_mae'):
        """Analyze which parameters have the most impact on performance."""
        if self.df is None or self.df.empty:
            return {}

        # Filter out rows where the metric is not finite
        finite_metric_df = self.df[np.isfinite(self.df[metric])].copy()
        if finite_metric_df.empty:
            return {}

        config_cols = [col for col in finite_metric_df.columns if col.startswith('config_')]
        importance_scores = {}

        for col in config_cols:
            param_name = col.replace('config_', '')
            if col not in finite_metric_df.columns: # Should not happen if config_cols from finite_metric_df
                continue

            # Ensure param column has more than one unique value to calculate importance
            if finite_metric_df[col].nunique(dropna=False) <= 1: # dropna=False to count NaN as a unique value if present
                importance_scores[param_name] = 0.0
                continue

            if finite_metric_df[col].dtype in ['int64', 'float64', 'int32', 'float32', 'bool']:
                if finite_metric_df[metric].nunique() > 1: # Metric also needs variance
                    correlation = abs(finite_metric_df[col].corr(finite_metric_df[metric]))
                    importance_scores[param_name] = correlation if pd.notna(correlation) else 0.0
                else:
                    importance_scores[param_name] = 0.0
            else: # Categorical
                # Treat as string for grouping to handle mixed types or Nones robustly
                groups = finite_metric_df.groupby(finite_metric_df[col].astype(str))[metric]
                if len(groups) > 1 and finite_metric_df[metric].nunique() > 1:
                    group_means = groups.mean()
                    group_vars = groups.var()

                    if group_means.isna().any() or not np.all(np.isfinite(group_means.dropna())):
                        importance_scores[param_name] = 0.0
                        continue

                    variance_between = group_means.var(ddof=0) # Population variance for means
                    variance_within = group_vars.fillna(0).mean() # Fill NaN var (e.g. single item group) with 0

                    if pd.notna(variance_between) and pd.notna(variance_within):
                        if variance_within > 1e-9: # Avoid division by zero or tiny numbers
                            importance_scores[param_name] = variance_between / variance_within
                        elif variance_between > 1e-9 : # If variance_within is ~0 but variance_between is not
                             importance_scores[param_name] = variance_between * 1e9 # Large number
                        else:
                            importance_scores[param_name] = 0.0 # Both are zero or near zero
                    else:
                        importance_scores[param_name] = 0.0
                else:
                    importance_scores[param_name] = 0.0

        return dict(sorted([item for item in importance_scores.items() if pd.notna(item[1])], key=lambda x: x[1], reverse=True))

    def plot_search_progress(self, save_path: Optional[str] = None):
        """Plot search progress over trials."""
        if self.df is None or self.df.empty:
            print("DataFrame not available for plotting search progress.")
            # Optionally plot a message
            fig, ax = plt.subplots(1,1)
            ax.text(0.5, 0.5, "No data to plot search progress.", ha="center", va="center", transform=ax.transAxes)
            ax.set_axis_off()
            if save_path: plt.savefig(save_path)
            plt.show()
            return

        plt.figure(figsize=(12, 8))

        df_finite_mae = self.df[np.isfinite(self.df['val_mae'])].copy()

        # Plot validation MAE over trials
        plt.subplot(2, 2, 1)
        if not df_finite_mae.empty:
            plt.plot(df_finite_mae['trial_id'], df_finite_mae['val_mae'], 'o-', alpha=0.7)
        else:
            plt.plot([], [], 'o-') # Empty plot
            plt.text(0.5, 0.5, "No finite MAE data", transform=plt.gca().transAxes, ha="center", va="center")
        plt.xlabel('Trial ID')
        plt.ylabel('Validation MAE')
        plt.title('Search Progress (Finite MAEs)')

        # Plot best MAE so far
        plt.subplot(2, 2, 2)
        if not df_finite_mae.empty:
            sorted_finite_df = df_finite_mae.sort_values('trial_id')
            if not sorted_finite_df.empty:
                 best_so_far = sorted_finite_df['val_mae'].cummin()
                 plt.plot(sorted_finite_df['trial_id'], best_so_far, 'r-', linewidth=2)
            else: # Should not be reached if df_finite_mae is not empty
                 plt.plot([], [], 'r-')
                 plt.text(0.5, 0.5, "No finite MAE data", transform=plt.gca().transAxes, ha="center", va="center")
        else:
            plt.plot([], [], 'r-')
            plt.text(0.5, 0.5, "No finite MAE data", transform=plt.gca().transAxes, ha="center", va="center")
        plt.xlabel('Trial ID')
        plt.ylabel('Best Validation MAE (Finite)')
        plt.title('Best Performance Over Time (Finite MAEs)')

        # Distribution of validation MAE
        plt.subplot(2, 2, 3)
        if not df_finite_mae.empty:
            plt.hist(df_finite_mae['val_mae'], bins=20, alpha=0.7, edgecolor='black')
        else:
            plt.text(0.5, 0.5, "No finite MAE data for histogram", transform=plt.gca().transAxes, ha="center", va="center")
        plt.xlabel('Validation MAE')
        plt.ylabel('Frequency')
        plt.title('Distribution of Performance (Finite MAEs)')

        # Training time vs performance
        plt.subplot(2, 2, 4)
        if not df_finite_mae.empty:
            plt.scatter(df_finite_mae['training_time'], df_finite_mae['val_mae'], alpha=0.7)
        else:
            plt.text(0.5, 0.5, "No finite MAE data for scatter plot", transform=plt.gca().transAxes, ha="center", va="center")
        plt.xlabel('Training Time (seconds)')
        plt.ylabel('Validation MAE')
        plt.title('Training Time vs Performance (Finite MAEs)')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def plot_parameter_effects(self, top_n: int = 5, save_path: Optional[str] = None):
        """Plot effects of top parameters on performance."""
        if self.df is None or self.df.empty:
            print("DataFrame not available for plotting parameter effects.")
            fig, ax = plt.subplots(1,1)
            ax.text(0.5, 0.5, "No data to plot parameter effects.", ha="center", va="center", transform=ax.transAxes)
            ax.set_axis_off()
            if save_path: plt.savefig(save_path)
            plt.show()
            return

        df_finite = self.df[np.isfinite(self.df['val_mae'])].copy()

        if df_finite.empty:
            print("No finite 'val_mae' data to plot parameter effects.")
            fig, ax = plt.subplots(1, 1)
            ax.text(0.5, 0.5, "No finite 'val_mae' data for parameter effects plot.",
                    transform=ax.transAxes, ha="center", va="center")
            ax.set_axis_off()
            if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            return

        importance = self.analyze_parameter_importance(metric='val_mae')
        top_params = list(importance.keys())[:top_n]

        if not top_params:
            print("No top parameters to plot (e.g., no variance after filtering).")
            fig, ax = plt.subplots(1, 1)
            ax.text(0.5, 0.5, "No top parameters to plot.",
                    transform=ax.transAxes, ha="center", va="center")
            ax.set_axis_off()
            if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.show()
            return

        num_plots = len(top_params)
        ncols = min(3, num_plots) if num_plots > 0 else 1
        nrows = (num_plots + ncols - 1) // ncols if num_plots > 0 else 1

        fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4.5 * nrows), squeeze=False)
        axes = axes.flatten()

        for i, param in enumerate(top_params):
            col_name = f'config_{param}'
            ax = axes[i]

            if col_name not in df_finite.columns:
                ax.text(0.5, 0.5, f"Param '{param}' not in data.", transform=ax.transAxes, ha="center", va="center")
                ax.set_title(f'{param} (Data N/A)')
                continue

            param_data = df_finite[col_name]
            metric_data = df_finite['val_mae']

            if param_data.dtype in ['int64', 'float64', 'int32', 'float32'] and param_data.nunique() > 5 : # Numeric scatter
                ax.scatter(param_data, metric_data, alpha=0.6, s=20)
            else: # Categorical, boolean, or few unique numerics -> box plot
                # Convert to string to handle mixed types or ensure discrete categories
                param_data_str = param_data.astype(str)
                unique_values = sorted(param_data_str.unique())

                data_by_value = [metric_data[param_data_str == val].dropna().values for val in unique_values]

                valid_data_by_value = [d for d in data_by_value if len(d) > 0]
                valid_labels = [unique_values[idx] for idx, d in enumerate(data_by_value) if len(d) > 0]

                if valid_data_by_value:
                    ax.boxplot(valid_data_by_value, labels=valid_labels, patch_artist=True, medianprops=dict(color="black"))
                else:
                    ax.text(0.5, 0.5, 'No data for boxplot', transform=ax.transAxes, ha="center", va="center")

            ax.set_xlabel(param)
            ax.set_ylabel('Validation MAE')
            ax.set_title(f'{param} (Importance: {importance.get(param, float("nan")):.3f})')
            ax.tick_params(axis='x', rotation=30) # Rotate x-labels if they are long

        for i in range(len(top_params), len(axes)): # Hide unused subplots
            axes[i].set_visible(False)

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

def run_hyperparameter_search(
    dataset,
    search_space: SearchSpace,
    strategy: str = "random",
    n_trials: int = 50,
    n_folds: int = 3,
    epochs: int = 100,
    device: torch.device = None,
    save_results: bool = True,
    results_path: str = "hyperparam_results.pkl",
    verbose: bool = True
) -> Tuple[List[Dict], SearchResultAnalyzer]:
    """
    Run comprehensive hyperparameter search for MEGAN.

    Args:
        dataset: PyTorch Geometric dataset
        search_space: SearchSpace object defining parameter ranges
        strategy: Search strategy ('random', 'grid', 'smart', 'bayesian')
        n_trials: Number of configurations to test
        n_folds: Number of cross-validation folds
        epochs: Training epochs per trial
        device: PyTorch device
        save_results: Whether to save results to file
        results_path: Path to save results
        verbose: Whether to print progress

    Returns:
        Tuple of (results_list, analyzer)
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize searcher
    searcher = HyperparameterSearcher(search_space, strategy)
    configs = searcher.generate_configs(n_trials)

    # Create cross-validation splits
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    fold_splits = list(kf.split(range(len(dataset))))

    results = []

    if verbose:
        print(f"Starting hyperparameter search with {len(configs)} configurations")
        print(f"Strategy: {strategy}, Folds: {n_folds}, Epochs per trial: {epochs}")
        print("-" * 80)

    for trial_id, config in enumerate(configs):
        if verbose:
            print(f"\nTrial {trial_id + 1}/{len(configs)}")
            print(f"Config: {config}")

        start_time = time.time()

        try:
            # Create MEGANConfig from hyperparameters
            megan_config = MEGANConfig("default")  # Start with default

            # Override with search parameters
            for key, value in config.items():
                if hasattr(megan_config, key):
                    setattr(megan_config, key, value)

            # Set training parameters
            megan_config.epochs = epochs

            # Import and run training (assuming the enhanced MEGAN code is available)
            try:

                fold_results = train_megan(
                    dataset, megan_config, fold_splits, device, verbose=False
                )
            except ImportError:
                # Fallback: simulate training results for demonstration
                fold_results = []
                for fold_idx in range(n_folds):
                    # Simulate validation MAE (lower is better)
                    # In reality, this would come from actual training
                    simulated_mae = np.random.exponential(0.1) + 0.05
                    fold_results.append({
                        'fold_idx': fold_idx,
                        'best_val_mae': simulated_mae,
                        'final_train_mae': simulated_mae * 0.8,
                    })

            # Aggregate results across folds
            val_maes = [r['best_val_mae'] for r in fold_results]
            train_maes = [r['final_train_mae'] for r in fold_results]

            avg_val_mae = np.mean(val_maes)
            avg_train_mae = np.mean(train_maes)
            std_val_mae = np.std(val_maes)

            training_time = time.time() - start_time

            # Store results
            result = {
                'trial_id': trial_id,
                'config': config,
                'val_mae': avg_val_mae,
                'val_mae_std': std_val_mae,
                'train_mae': avg_train_mae,
                'training_time': training_time,
                'fold_results': fold_results,
                'success': True
            }

            results.append(result)

            # Update best configuration
            if avg_val_mae < searcher.best_score:
                searcher.best_score = avg_val_mae
                searcher.best_config = config.copy()

            if verbose:
                print(f"  Val MAE: {avg_val_mae:.5f} ± {std_val_mae:.5f}")
                print(f"  Train MAE: {avg_train_mae:.5f}")
                print(f"  Time: {training_time:.1f}s")
                print(f"  Best so far: {searcher.best_score:.5f}")

        except Exception as e:
            if verbose:
                print(f"  Trial failed: {str(e)}")

            result = {
                'trial_id': trial_id,
                'config': config,
                'val_mae': float('inf'),
                'val_mae_std': float('inf'),
                'train_mae': float('inf'),
                'training_time': time.time() - start_time,
                'fold_results': [],
                'success': False,
                'error': str(e)
            }
            results.append(result)

    # Save results
    if save_results:
        with open(results_path, 'wb') as f:
            pickle.dump({
                'results': results,
                'search_space': search_space,
                'strategy': strategy,
                'best_config': searcher.best_config,
                'best_score': searcher.best_score
            }, f)

        if verbose:
            print(f"\nResults saved to {results_path}")

    # Create analyzer
    analyzer = SearchResultAnalyzer(results)

    if verbose:
        print(f"\n" + "="*80)
        print("HYPERPARAMETER SEARCH COMPLETE")
        print(f"="*80)
        print(f"Total trials: {len(results)}")
        print(f"Successful trials: {sum(1 for r in results if r['success'])}")
        print(f"Best validation MAE: {searcher.best_score:.5f}")
        print(f"Best configuration: {searcher.best_config}")

        # Show top 5 configurations
        print(f"\nTop 5 configurations:")
        top_configs = analyzer.get_top_configs(5)
        for i, config in enumerate(top_configs, 1):
            print(f"  {i}. Val MAE: {config['val_mae']:.5f}")
            config_params = {k.replace('config_', ''): v for k, v in config.items()
                           if k.startswith('config_')}
            print(f"     {config_params}")

    return results, analyzer

# Example usage and testing

# Define search space
if __name__ == "__main__":
    search_space = SearchSpace(
        hidden_channels=[32, 64, 128],
        num_layers=[3, 4, 5],
        K=[2, 3],
        learning_rate=[1e-4, 5e-4, 1e-3],
        dropout=[0.1, 0.15, 0.2],
        use_edge_features=[True, False]
    )

    # Run search
    results, analyzer = run_hyperparameter_search(
        dataset=dataset,
        search_space=search_space,
        strategy="smart",
        n_trials=20,
        n_folds=3,
        epochs=50,
        verbose=True
    )

    # Analyze results
    print("\nParameter importance analysis:")
    importance = analyzer.analyze_parameter_importance()
    for param, score in importance.items():
        print(f"  {param}: {score:.4f}")

    # Plot results (uncomment if you have matplotlib)
    analyzer.plot_search_progress()
    analyzer.plot_parameter_effects()

    from sklearn.model_selection import KFold
    import numpy as np

    # =============================================================================
    # 2) KFold split & DataLoader
    # =============================================================================

    kf = KFold(n_splits=2, shuffle=True, random_state=42)
    all_indices = np.arange(len(dataset))
    fold_splits = [(train_idx, test_idx) for train_idx, test_idx in kf.split(all_indices)]

    # # =============================================================================
    # # 5) Final Summary
    # # =============================================================================
    # print("\n=== Final Summary Over Folds ===")
    # for i, (tr_loss, val_loss) in enumerate(zip(fold_train_losses, fold_val_losses)):
    #     print(f"Fold {i}: Train MAE ≈ {tr_loss:.5f}, Best Val MAE ≈ {val_loss:.5f}")

    # # Example: Inspect first 5 edges’ and nodes’ importance from fold 0
    # if fold_test_importances:
    #     Eim0, Vim0 = fold_test_importances[0]
    #     print("\nFold 0 — First 5 edges’ importance (Eim):")
    #     print(Eim0[:5, :])
    #     print("Fold 0 — First 5 nodes’ importance (Vim):")
    #     print(Vim0[:5, :])

    # Load your dataset
    from torch_geometric.datasets import MoleculeNet
    from torch_geometric.transforms import ToUndirected

    dataset = MoleculeNet(root="data/ESOL", name="ESOL", transform=ToUndirected())

    # Create KFold splits
    from sklearn.model_selection import KFold
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    fold_splits = list(kf.split(range(len(dataset))))

    # Choose configuration
    config = MEGANConfig("edge_focused")  # or "default", "large", "small"

    # Train
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    results = train_megan(
        dataset, config, fold_splits, device,
        verbose=True
    )

    # Analyze results
    avg_val_mae = np.mean([r['best_val_mae'] for r in results])
    print(f"Average validation MAE across folds: {avg_val_mae:.5f}")

################################################################################
#                           Enhanced Visualization Functions                   #
################################################################################

    import matplotlib.pyplot as plt

def plot_loss_dynamics(losses, save_path=None):
    """
    Plot training loss dynamics showing all components.
    """
    plt.figure(figsize=(15, 10))

    epochs = range(len(losses["total"]))

    # Total loss
    plt.subplot(2, 3, 1)
    plt.plot(epochs, losses["total"], 'b-', label='Total Loss')
    plt.plot(epochs, losses["val"], 'r-', label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Total Training vs Validation Loss')
    plt.legend()
    plt.grid(True)

    # Prediction loss
    plt.subplot(2, 3, 2)
    plt.plot(epochs, losses["pred"], 'g-', label='Prediction Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Prediction Loss')
    plt.legend()
    plt.grid(True)

    # Sparsity loss
    plt.subplot(2, 3, 3)
    plt.plot(epochs, losses["sparsity"], 'orange', label='Sparsity Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Sparsity Regularization Loss')
    plt.legend()
    plt.grid(True)

    # Decorrelation loss
    plt.subplot(2, 3, 4)
    plt.plot(epochs, losses["decor"], 'purple', label='Decorrelation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Channel Decorrelation Loss')
    plt.legend()
    plt.grid(True)

    # Combined regularization
    plt.subplot(2, 3, 5)
    combined_reg = [s + d for s, d in zip(losses["sparsity"], losses["decor"])]
    plt.plot(epochs, combined_reg, 'm-', label='Combined Regularization')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Combined Regularization Loss')
    plt.legend()
    plt.grid(True)

    # Loss ratios
    plt.subplot(2, 3, 6)
    pred_ratio = [p/t for p, t in zip(losses["pred"], losses["total"])]
    reg_ratio = [r/t for r, t in zip(combined_reg, losses["total"])]
    plt.plot(epochs, pred_ratio, 'g-', label='Prediction/Total')
    plt.plot(epochs, reg_ratio, 'm-', label='Regularization/Total')
    plt.xlabel('Epoch')
    plt.ylabel('Ratio')
    plt.title('Loss Component Ratios')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def plot_channel_explanation(Vim, batch, save_path=None):
    """
    Visualize node explanations across channels.

    Args:
        Vim: Node importance scores [V, K]
        batch: Batch indices [V]
        node_labels: Optional node labels
        save_path: Optional save path
    """
    K = Vim.shape[1]
    V = Vim.shape[0]

    if isinstance(Vim, torch.Tensor):
        Vim = Vim.detach().cpu().numpy()
    if isinstance(batch, torch.Tensor):
        batch = batch.detach().cpu().numpy()

    fig, axs = plt.subplots(1, K, figsize=(5*K, 6))
    if K == 1:
        axs = [axs]

    for k in range(K):
        # Bar plot for this channel
        bars = axs[k].bar(range(V), Vim[:, k], alpha=0.7,
                         color=plt.cm.Set1(k))

        axs[k].set_xlabel('Node Index')
        axs[k].set_ylabel('Importance Score')
        axs[k].set_title(f'Explanation Channel {k+1}')
        axs[k].grid(True, alpha=0.3)

        # Add batch separators
        unique_batches = np.unique(batch)
        if len(unique_batches) > 1:
            batch_boundaries = []
            for b in unique_batches[:-1]:
                boundary = np.where(batch == b)[0][-1] + 0.5
                batch_boundaries.append(boundary)
                axs[k].axvline(x=boundary, color='red', linestyle='--', alpha=0.5)

        # Highlight top nodes
        top_indices = np.argsort(Vim[:, k])[-3:]  # Top 3
        for idx in top_indices:
            bars[idx].set_color('red')
            bars[idx].set_alpha(0.8)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def plot_fidelity_analysis(fidelity_stats, save_path=None):
    """
    Plot explanation fidelity analysis.
    """
    channels = list(fidelity_stats.keys())
    means = [fidelity_stats[ch]['mean'] for ch in channels]
    stds = [fidelity_stats[ch]['std'] for ch in channels]
    medians = [fidelity_stats[ch]['median'] for ch in channels]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Bar plot with error bars
    x_pos = range(len(channels))
    ax1.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue')
    ax1.set_xlabel('Explanation Channel')
    ax1.set_ylabel('Fidelity Score (Mean ± Std)')
    ax1.set_title('Explanation Fidelity by Channel')
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels([ch.replace('channel_', 'Ch ') for ch in channels])
    ax1.grid(True, alpha=0.3)

    # Box plot comparison
    ax2.scatter(x_pos, means, color='red', s=50, label='Mean', zorder=3)
    ax2.scatter(x_pos, medians, color='blue', s=50, label='Median', zorder=3)
    for i, (m, s) in enumerate(zip(means, stds)):
        ax2.plot([i, i], [m-s, m+s], 'k-', alpha=0.5)

    ax2.set_xlabel('Explanation Channel')
    ax2.set_ylabel('Fidelity Score')
    ax2.set_title('Fidelity Statistics Comparison')
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels([ch.replace('channel_', 'Ch ') for ch in channels])
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
if __name__ == "__main__":
    # Plot loss dynamics for first fold
    if results and 'losses' in results[0]:
        plot_loss_dynamics(results[0]['losses'])

    # Plot fidelity analysis
    if results and 'fidelity_stats' in results[0]:
        plot_fidelity_analysis(results[0]['fidelity_stats'])

    ################################################################################
    #                              Evaluation Metrics                              #
    ################################################################################
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np

def evaluate_megan(results: list, dataset, config: MEGANConfig, device: torch.device):
    """
    Comprehensive evaluation of MEGAN model performance and explanations.

    Args:
        results: List of fold results from train_megan()
        dataset: Full PyG dataset
        config: MEGAN configuration used
        device: Torch device

    Returns:
        Dictionary containing all evaluation metrics and visualizations
    """
    from torch.utils.data import Subset
    from torch_geometric.loader import DataLoader

    # Initialize metrics storage
    all_y_true = []
    all_y_pred = []
    fold_metrics = []
    importance_analysis = {
        'node_importances': [],
        'edge_importances': [],
        'explanation_consistency': []
    }

    for fold_result in results:
        fold_idx = fold_result['fold_idx']
        test_idx = fold_result.get('test_indices', range(len(dataset)))  # Fallback if not stored

        # Get test subset
        test_subset = Subset(dataset, test_idx)
        test_loader = DataLoader(test_subset, batch_size=config.batch_size, shuffle=False)

        # Rebuild model (or could load from checkpoint)
        model_kwargs = {
            "in_channels": dataset.num_node_features,
            "hidden_channels": config.hidden_channels,
            "out_channels": 1,
            "edge_dim": getattr(dataset, 'num_edge_features', 0),
            "num_layers": config.num_layers,
            "K": config.K,
            "heads_gat": config.heads_gat,
            "use_edge_features": config.use_edge_features,
            "dropout": 0.0,  # Disable dropout for evaluation
            "layer_norm": config.layer_norm,
            "residual": config.residual,
        }

        model = MEGANCore(**model_kwargs).to(device)
        model.eval()

        # Storage for this fold
        fold_y_true = []
        fold_y_pred = []
        fold_eim = []
        fold_vim = []

        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device)
                x = batch.x.float()
                edge_index = batch.edge_index
                batch_mask = batch.batch
                y_true = batch.y.view(-1, 1)

                edge_attr = None
                if config.use_edge_features and hasattr(batch, 'edge_attr') and batch.edge_attr is not None:
                    edge_attr = batch.edge_attr.float()

                y_pred, Eim, Vim = model(x, edge_index, batch_mask, edge_attr)

                fold_y_true.append(y_true.cpu().numpy())
                fold_y_pred.append(y_pred.cpu().numpy())

                # Concatenate Eim/Vim across graphs (they’re already per-graph lists in `results`)
                fold_eim.append(Eim.cpu().numpy())
                fold_vim.append(Vim.cpu().numpy())

        # Concatenate results (per-graph flatten, for metrics)
        y_true = np.concatenate(fold_y_true)
        y_pred = np.concatenate(fold_y_pred)
        eim = np.concatenate(fold_eim)
        vim = np.concatenate(fold_vim)

        # Calculate metrics
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)

        fold_metrics.append({
            'fold': fold_idx,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'num_samples': len(y_true)
        })

        # Store for overall analysis
        all_y_true.append(y_true)
        all_y_pred.append(y_pred)
        importance_analysis['node_importances'].append(vim)
        importance_analysis['edge_importances'].append(eim)

    # Aggregate results
    all_y_true = np.concatenate(all_y_true)
    all_y_pred = np.concatenate(all_y_pred)

    # Overall metrics
    overall_metrics = {
        'mae': mean_absolute_error(all_y_true, all_y_pred),
        'rmse': np.sqrt(mean_squared_error(all_y_true, all_y_pred)),
        'r2': r2_score(all_y_true, all_y_pred),
        'std_mae': np.std([f['mae'] for f in fold_metrics]),
        'std_rmse': np.std([f['rmse'] for f in fold_metrics]),
        'std_r2': np.std([f['r2'] for f in fold_metrics]),
    }

    # Importance analysis
    node_importances = np.concatenate(importance_analysis['node_importances'])
    edge_importances = np.concatenate(importance_analysis['edge_importances'])

    # Generate visualizations
    visualizations = generate_visualizations(
        all_y_true, all_y_pred,
        node_importances, edge_importances,
        config.K
    )

    return {
        'overall_metrics': overall_metrics,
        'fold_metrics': fold_metrics,
        'importance_analysis': {
            'node_importances_mean': node_importances.mean(axis=0),
            'node_importances_std': node_importances.std(axis=0),
            'edge_importances_mean': edge_importances.mean(axis=0),
            'edge_importances_std': edge_importances.std(axis=0),
        },
        'visualizations': visualizations,
        'config': vars(config)  # Convert to dict for logging
    }

def generate_visualizations(y_true, y_pred, node_importances, edge_importances, K):
    """Generate evaluation visualizations."""
    plt.figure(figsize=(15, 10))
    visualizations = {}

    # 1. Prediction vs True values scatter plot
    plt.subplot(2, 2, 1)
    sns.regplot(x=y_true.flatten(), y=y_pred.flatten(), scatter_kws={'alpha':0.4})
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.title('Prediction vs True Values')
    visualizations['prediction_plot'] = plt.gcf()

    # 2. Error distribution
    plt.subplot(2, 2, 2)
    errors = y_pred.flatten() - y_true.flatten()
    sns.histplot(errors, kde=True)
    plt.xlabel('Prediction Error')
    plt.title('Error Distribution')
    visualizations['error_distribution'] = plt.gcf()

    # 3. Node importance distribution per channel
    plt.subplot(2, 2, 3)
    for k in range(K):
        sns.kdeplot(node_importances[:, k], label=f'Channel {k+1}')
    plt.xlabel('Node Importance Score')
    plt.title('Node Importance Distribution')
    plt.legend()
    visualizations['node_importance_dist'] = plt.gcf()

    # 4. Edge importance distribution per channel
    plt.subplot(2, 2, 4)
    for k in range(K):
        sns.kdeplot(edge_importances[:, k], label=f'Channel {k+1}')
    plt.xlabel('Edge Importance Score')
    plt.title('Edge Importance Distribution')
    plt.legend()
    visualizations['edge_importance_dist'] = plt.gcf()

    plt.tight_layout()
    return visualizations

################################################################################
#                              Example Molecule Visualization                  #
################################################################################
import matplotlib.pyplot as plt
import networkx as nx
from rdkit import Chem
from rdkit.Chem import Draw
from IPython.display import display
import numpy as np

def visualize_molecule_with_networkx(data, node_importances, edge_importances,
                                     channel_idx=0):
    smiles = getattr(data, 'smiles', None)
    if smiles is None:
        raise ValueError("Data object must contain a SMILES string.")
    mol = Chem.MolFromSmiles(smiles)
    Chem.Kekulize(mol, clearAromaticFlags=True)

    node_scores = node_importances[:, channel_idx]
    edge_scores = edge_importances[:, channel_idx]
    edge_index = data.edge_index.cpu().numpy()

    # Normalize scores robustly
    def normalize(x):
        x = np.array(x)
        return (x - x.min()) / (x.max() - x.min() + 1e-8) if x.max() > x.min() else x

    node_scores = normalize(node_scores)
    edge_scores = normalize(edge_scores)

    # Build graph
    G = nx.Graph()
    for i, atom in enumerate(mol.GetAtoms()):
        G.add_node(i, atom=atom.GetSymbol(), importance=node_scores[i])

    bond_importances = {}
    for idx, (i, j) in enumerate(edge_index.T):
        if idx < len(edge_scores):
            bond = mol.GetBondBetweenAtoms(int(i), int(j))
            if bond is not None:
                bond_idx = bond.GetIdx()
                bond_importances.setdefault(bond_idx, []).append(float(edge_scores[idx]))

    for bond in mol.GetBonds():
        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
        bond_idx = bond.GetIdx()
        avg_imp = np.mean(bond_importances[bond_idx]) if bond_idx in bond_importances else 0.0
        G.add_edge(i, j, importance=avg_imp)

    pos = nx.spring_layout(G, seed=42)

    fig, ax = plt.subplots(figsize=(8, 6))

    node_vals = [G.nodes[n]['importance'] for n in G.nodes()]
    edge_vals = [G.edges[e]['importance'] for e in G.edges()]

    node_cmap = plt.get_cmap('Reds')
    edge_cmap = plt.get_cmap('Blues')

    nodes = nx.draw_networkx_nodes(G, pos, node_color=node_vals, cmap=node_cmap, node_size=700, ax=ax)
    edges = nx.draw_networkx_edges(G, pos, edge_color=edge_vals, edge_cmap=edge_cmap, width=2, ax=ax)
    nx.draw_networkx_labels(G, pos, labels={n: G.nodes[n]['atom'] for n in G.nodes()}, font_size=12, ax=ax)

    # Add colorbars
    sm_node = plt.cm.ScalarMappable(cmap=node_cmap, norm=plt.Normalize(vmin=0, vmax=1))
    sm_node._A = []
    cbar_node = plt.colorbar(sm_node, ax=ax, orientation='vertical', fraction=0.046, pad=0.02)
    cbar_node.set_label("Atom Importance")

    sm_edge = plt.cm.ScalarMappable(cmap=edge_cmap, norm=plt.Normalize(vmin=0, vmax=1))
    sm_edge._A = []
    cbar_edge = plt.colorbar(sm_edge, ax=ax, orientation='vertical', fraction=0.046, pad=0.08)
    cbar_edge.set_label("Bond Importance")

    plt.title(f"NetworkX Visualization (Channel {channel_idx})")
    plt.axis('off')
    plt.tight_layout()
    plt.show()

################################################################################
#                    Solubility Interpretation Functions                       #
################################################################################


def interpret_solubility_prediction(prediction_value, node_importances, edge_importances, data):
    """
    Generate a comprehensive solubility conclusion based on MEGAN results.

    Args:
        prediction_value: Predicted log solubility value
        node_importances: Node importance scores [num_nodes, K]
        edge_importances: Edge importance scores [num_edges, K]
        data: Molecule data object

    Returns:
        Dictionary with solubility interpretation
    """
    # Convert log solubility to interpretable units
    log_sol = float(prediction_value)
    mol_per_liter = 10 ** log_sol
    mg_per_ml = mol_per_liter * data.mol_weight if hasattr(data, 'mol_weight') else None

    # Solubility classification
    if log_sol > -1:
        solubility_class = "Highly Soluble"
        description = "Very water-soluble, dissolves readily"
        color = "green"
    elif log_sol > -2:
        solubility_class = "Moderately Soluble"
        description = "Good water solubility"
        color = "lightgreen"
    elif log_sol > -3:
        solubility_class = "Poorly Soluble"
        description = "Limited water solubility"
        color = "orange"
    elif log_sol > -4:
        solubility_class = "Very Poorly Soluble"
        description = "Poor water solubility"
        color = "red"
    else:
        solubility_class = "Practically Insoluble"
        description = "Extremely poor water solubility"
        color = "darkred"

    # Analyze molecular features contributing to solubility
    interpretation = analyze_solubility_features(node_importances, edge_importances, data)

    return {
        'prediction': {
            'log_solubility': log_sol,
            'molarity': mol_per_liter,
            'mg_per_ml': mg_per_ml,
            'class': solubility_class,
            'description': description,
            'color': color
        },
        'molecular_analysis': interpretation
    }

def analyze_solubility_features(node_importances, edge_importances, data):
    """
    Analyze which molecular features contribute to solubility prediction.
    """
    try:
        from rdkit import Chem
        from rdkit.Chem import Descriptors, rdMolDescriptors

        smiles = getattr(data, 'smiles', None)
        if smiles:
            mol = Chem.MolFromSmiles(smiles)
        else:
            return {"error": "No SMILES available for analysis"}
    except:
        return {"error": "RDKit not available for molecular analysis"}

    # Get top important atoms for each channel
    K = node_importances.shape[1]
    top_atoms = {}

    for k in range(K):
        top_indices = np.argsort(node_importances[:, k])[-3:]  # Top 3 atoms
        top_atoms[f'channel_{k}'] = []

        for idx in top_indices:
            if idx < mol.GetNumAtoms():
                atom = mol.GetAtomWithIdx(int(idx))
                top_atoms[f'channel_{k}'].append({
                    'index': int(idx),
                    'symbol': atom.GetSymbol(),
                    'importance': float(node_importances[idx, k])
                })

    # Calculate molecular descriptors related to solubility
    descriptors = {}
    try:
        descriptors['molecular_weight'] = Descriptors.MolWt(mol)
        descriptors['logp'] = Descriptors.MolLogP(mol)  # Lipophilicity
        descriptors['hbd'] = rdMolDescriptors.CalcNumHBD(mol)  # H-bond donors
        descriptors['hba'] = rdMolDescriptors.CalcNumHBA(mol)  # H-bond acceptors
        descriptors['tpsa'] = rdMolDescriptors.CalcTPSA(mol)  # Topological polar surface area
        descriptors['rotatable_bonds'] = rdMolDescriptors.CalcNumRotatableBonds(mol)
    except:
        descriptors = {"error": "Could not calculate descriptors"}

    # Generate interpretation
    interpretation = {
        'top_important_atoms': top_atoms,
        'molecular_descriptors': descriptors,
        'solubility_factors': generate_solubility_explanation(descriptors)
    }

    return interpretation

def generate_solubility_explanation(descriptors):
    """
    Generate human-readable explanation of solubility factors.
    """
    if 'error' in descriptors:
        return ["Could not analyze molecular features"]

    explanations = []

    # Molecular weight effect
    mw = descriptors.get('molecular_weight', 0)
    if mw > 500:
        explanations.append("⚠️ High molecular weight (>500 Da) reduces solubility")
    elif mw < 200:
        explanations.append("✅ Low molecular weight (<200 Da) favors solubility")

    # LogP effect (lipophilicity)
    logp = descriptors.get('logp', 0)
    if logp > 3:
        explanations.append("⚠️ High lipophilicity (LogP > 3) reduces water solubility")
    elif logp < 1:
        explanations.append("✅ Low lipophilicity (LogP < 1) favors water solubility")

    # Hydrogen bonding
    hbd = descriptors.get('hbd', 0)
    hba = descriptors.get('hba', 0)
    if hbd + hba > 10:
        explanations.append("⚠️ Many H-bond sites may reduce solubility due to crystal packing")
    elif hbd + hba > 5:
        explanations.append("✅ Good H-bonding capability enhances water solubility")
    elif hbd + hba < 2:
        explanations.append("⚠️ Limited H-bonding reduces water interaction")

    # Polar surface area
    tpsa = descriptors.get('tpsa', 0)
    if tpsa > 140:
        explanations.append("⚠️ Large polar surface area may reduce membrane permeability")
    elif tpsa > 60:
        explanations.append("✅ Moderate polar surface area balances solubility and permeability")
    else:
        explanations.append("⚠️ Small polar surface area reduces water interaction")

    # Flexibility
    rot_bonds = descriptors.get('rotatable_bonds', 0)
    if rot_bonds > 10:
        explanations.append("⚠️ High flexibility may reduce solubility due to entropy penalty")
    elif rot_bonds > 5:
        explanations.append("➡️ Moderate flexibility allows conformational adaptation")

    return explanations

def visualize_solubility_conclusion(data, node_importances, edge_importances, prediction_value, channel_idx=0, show_molecular_plot=False):
    """
    Enhanced visualization with solubility conclusion - CORRECTED VERSION.
    """
    # Get solubility interpretation
    interpretation = interpret_solubility_prediction(prediction_value, node_importances, edge_importances, data)

    # Only show molecular visualization if explicitly requested
    if show_molecular_plot:
        print(f"\n=== Molecular Structure Visualization (Channel {channel_idx}) ===")
        visualize_molecule_with_networkx(data, node_importances, edge_importances, channel_idx)

    # Print the solubility conclusion as TEXT (not as plot)
    pred_info = interpretation['prediction']
    mol_analysis = interpretation['molecular_analysis']

    print(f"\n{'='*60}")
    print(f"🧪 SOLUBILITY ANALYSIS REPORT")
    print(f"{'='*60}")

    print(f"\n📊 PREDICTION RESULTS:")
    print(f"   • Solubility Class: {pred_info['class']}")
    print(f"   • Description: {pred_info['description']}")
    print(f"   • Predicted log(Solubility): {pred_info['log_solubility']:.3f} log(mol/L)")
    print(f"   • Predicted Solubility: {pred_info['molarity']:.2e} mol/L")

    if 'molecular_descriptors' in mol_analysis and 'error' not in mol_analysis['molecular_descriptors']:
        descriptors = mol_analysis['molecular_descriptors']
        print(f"\n🔬 KEY MOLECULAR FEATURES:")
        print(f"   • Molecular Weight: {descriptors.get('molecular_weight', 'N/A'):.1f} Da")
        print(f"   • LogP (Lipophilicity): {descriptors.get('logp', 'N/A'):.2f}")
        print(f"   • H-bond Donors: {descriptors.get('hbd', 'N/A')}")
        print(f"   • H-bond Acceptors: {descriptors.get('hba', 'N/A')}")
        print(f"   • Topological Polar Surface Area: {descriptors.get('tpsa', 'N/A'):.1f} Ų")
        print(f"   • Rotatable Bonds: {descriptors.get('rotatable_bonds', 'N/A')}")

    if 'solubility_factors' in mol_analysis:
        print(f"\n💡 SOLUBILITY FACTORS:")
        for factor in mol_analysis['solubility_factors']:
            print(f"   {factor}")

    if 'top_important_atoms' in mol_analysis:
        print(f"\n⭐ MOST IMPORTANT ATOMS (Channel {channel_idx}):")
        channel_key = f'channel_{channel_idx}'
        if channel_key in mol_analysis['top_important_atoms']:
            for atom_info in mol_analysis['top_important_atoms'][channel_key]:
                print(f"   • {atom_info['symbol']} (atom {atom_info['index']}): {atom_info['importance']:.3f}")

    print(f"{'='*60}")

    return interpretation

################################################################################
#                    MolT5-Enhanced Solubility Interpretation                 #
################################################################################

def create_molt5_prompt(data, node_importances, edge_importances, prediction_value, channel_idx=0):
    """
    Create a structured prompt for MolT5 based on MEGAN's results.

    Args:
        data: Molecule data object with SMILES
        node_importances: Node importance scores [num_nodes, K]
        edge_importances: Edge importance scores [num_edges, K]
        prediction_value: Predicted log solubility value
        channel_idx: Which explanation channel to focus on

    Returns:
        Formatted prompt string for MolT5
    """
    try:
        # Get SMILES
        smiles = getattr(data, 'smiles', None)
        if smiles is None:
            return None, "No SMILES available"

        # Analyze MEGAN's explanation
        megan_analysis = extract_megan_explanation(
            data, node_importances, edge_importances, channel_idx
        )

        # Create solubility classification
        log_sol = float(prediction_value)
        if log_sol > -1:
            solubility_class = "highly soluble"
            water_interaction = "dissolves readily in water"
        elif log_sol > -2:
            solubility_class = "moderately soluble"
            water_interaction = "has good water solubility"
        elif log_sol > -3:
            solubility_class = "poorly soluble"
            water_interaction = "has limited water solubility"
        elif log_sol > -4:
            solubility_class = "very poorly soluble"
            water_interaction = "has poor water solubility"
        else:
            solubility_class = "practically insoluble"
            water_interaction = "barely dissolves in water"

        # Construct the prompt
        molt5_prompt = (
            f"Analyze the solubility of molecule {smiles}. "
            f"MEGAN neural network predicted logS solubility as {log_sol:.3f} mol/L, "
            f"meaning this molecule is {solubility_class} and {water_interaction}. "
            f"MEGAN's explainable AI analysis highlights that {megan_analysis['key_feature']} "
            f"is the most important structural factor. "
            f"Specifically, {megan_analysis['detailed_explanation']} "
            f"The attention mechanism shows {megan_analysis['attention_insight']}. "
            f"Based on this MEGAN analysis, provide a comprehensive molecular-level "
            f"explanation of why this molecule has its predicted solubility, focusing on "
            f"intermolecular forces, molecular structure, and water interactions."
        )

        return molt5_prompt, None

    except Exception as e:
        return None, f"Error creating prompt: {str(e)}"

def extract_megan_explanation(data, node_importances, edge_importances, channel_idx=0):
    """
    Extract key insights from MEGAN's attention weights and importance scores.

    Returns:
        Dictionary with structured explanation data
    """
    try:
        from rdkit import Chem

        smiles = getattr(data, 'smiles', None)
        if smiles is None:
            return {"error": "No SMILES available"}

        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return {"error": "Invalid SMILES"}

        # Find most important atom
        channel_node_scores = node_importances[:, channel_idx]
        most_important_atom_idx = np.argmax(channel_node_scores)
        most_important_score = channel_node_scores[most_important_atom_idx]

        if most_important_atom_idx < mol.GetNumAtoms():
            atom = mol.GetAtomWithIdx(int(most_important_atom_idx))
            atom_symbol = atom.GetSymbol()

            # Generate explanation based on atom type (simplified call)
            explanation_data = generate_atom_explanation(atom)

            # Analyze edge importance for context
            edge_context = analyze_edge_importance(
                data, edge_importances, channel_idx, most_important_atom_idx
            )

            return {
                'key_feature': f"the {atom_symbol} atom at position {most_important_atom_idx}",
                'detailed_explanation': explanation_data['mechanism'],
                'attention_insight': edge_context['bond_analysis'],
                'atom_symbol': atom_symbol,
                'importance_score': float(most_important_score)
            }
        else:
            return {"error": "Atom index out of range"}

    except Exception as e:
        return {"error": f"Analysis failed: {str(e)}"}

def generate_atom_explanation(atom):
    """
    Generate chemical explanation based on atom type.

    Args:
        atom: RDKit atom object

    Returns:
        Dictionary with mechanism explanation
    """
    symbol = atom.GetSymbol()

    explanations = {
        'O': {
            'mechanism': f"the oxygen atom (electronegativity 3.44) creates strong polar bonds and can form up to 2 hydrogen bonds with water molecules as both donor and acceptor",
            'context': "Oxygen-containing functional groups like hydroxyl (-OH), carbonyl (C=O), or ether (C-O-C) are key hydrophilic features"
        },
        'N': {
            'mechanism': f"the nitrogen atom (electronegativity 3.04) can act as hydrogen bond acceptor and sometimes donor, creating favorable water interactions",
            'context': "Nitrogen in amines, amides, or heterocycles contributes significantly to aqueous solubility"
        },
        'S': {
            'mechanism': f"the sulfur atom provides moderate polarity and can participate in weak hydrogen bonding or dipole interactions",
            'context': "Sulfur atoms in thiols, sulfides, or sulfonates affect molecular polarity"
        },
        'C': {
            'mechanism': f"this carbon atom appears to be in a critical structural position affecting molecular shape or connecting polar groups",
            'context': "Carbon atoms gain importance when they bridge polar regions or affect molecular flexibility"
        }
    }

    default_explanation = {
        'mechanism': f"the {symbol} atom plays a crucial role in determining molecular polarity and shape",
        'context': f"{symbol} atoms can influence solubility through electronic effects and steric considerations"
    }

    return explanations.get(symbol, default_explanation)

def analyze_edge_importance(data, edge_importances, channel_idx, focus_atom_idx):
    """
    Analyze which bonds are most important according to MEGAN.
    """
    edge_index = data.edge_index.cpu().numpy()
    channel_edge_scores = edge_importances[:, channel_idx]

    # Find edges connected to the most important atom
    connected_edges = []
    for i, (src, dst) in enumerate(edge_index.T):
        if i < len(channel_edge_scores) and (src == focus_atom_idx or dst == focus_atom_idx):
            connected_edges.append({
                'edge_idx': i,
                'atoms': (int(src), int(dst)),
                'importance': float(channel_edge_scores[i])
            })

    if connected_edges:
        # Find most important bond
        most_important_bond = max(connected_edges, key=lambda x: x['importance'])
        bond_analysis = (f"strongest attention on the bond connecting atoms "
                        f"{most_important_bond['atoms'][0]} and {most_important_bond['atoms'][1]} "
                        f"(importance: {most_important_bond['importance']:.3f})")
    else:
        bond_analysis = "distributed attention across multiple molecular regions"

    return {'bond_analysis': bond_analysis}

def create_molt5_prompt(data, node_importances, edge_importances, prediction_value, channel_idx=0):
    """
    Create a more detailed and structured prompt for MolT5.
    """
    try:
        from rdkit import Chem

        # Get SMILES and basic info
        smiles = getattr(data, 'smiles', None)
        if smiles is None:
            return None, "No SMILES available"

        # Parse molecule for additional context
        mol = Chem.MolFromSmiles(smiles)
        if mol is not None:
            num_atoms = mol.GetNumAtoms()
            num_bonds = mol.GetNumBonds()
            molecular_formula = Chem.rdMolDescriptors.CalcMolFormula(mol)
        else:
            num_atoms = "unknown"
            num_bonds = "unknown"
            molecular_formula = "unknown"

        # Analyze MEGAN's explanation
        megan_analysis = extract_megan_explanation(
            data, node_importances, edge_importances, channel_idx
        )

        # Create detailed solubility classification
        log_sol = float(prediction_value)
        if log_sol > -1:
            solubility_class = "highly soluble"
            water_interaction = "dissolves readily in water with extensive hydrogen bonding"
            thermodynamic_desc = "enthalpically favorable dissolution"
        elif log_sol > -2:
            solubility_class = "moderately soluble"
            water_interaction = "shows good water solubility with balanced polar-nonpolar interactions"
            thermodynamic_desc = "moderately favorable solvation energetics"
        elif log_sol > -3:
            solubility_class = "poorly soluble"
            water_interaction = "has limited water solubility due to hydrophobic character"
            thermodynamic_desc = "unfavorable water partitioning"
        elif log_sol > -4:
            solubility_class = "very poorly soluble"
            water_interaction = "exhibits poor water solubility with minimal polar interactions"
            thermodynamic_desc = "strong preference for organic phases"
        else:
            solubility_class = "practically insoluble"
            water_interaction = "barely dissolves in water due to hydrophobic bulk"
            thermodynamic_desc = "highly unfavorable aqueous solvation"

        # Construct enhanced prompt
        enhanced_prompt = f"""Analyze the aqueous solubility of the molecular compound with SMILES: {smiles}

Molecular Properties:
- Formula: {molecular_formula}
- Structure: {num_atoms} atoms, {num_bonds} bonds
- Predicted logS: {log_sol:.3f} mol/L ({solubility_class})
- Solvation behavior: {water_interaction}
- Thermodynamics: {thermodynamic_desc}

MEGAN Neural Network Analysis:
- Key structural feature: {megan_analysis.get('key_feature', 'structural region')}
- Mechanistic role: {megan_analysis.get('detailed_explanation', 'affects molecular interactions')}
- Attention focus: {megan_analysis.get('attention_insight', 'distributed molecular attention')}
- Channel {channel_idx} perspective

Please provide a comprehensive molecular-level explanation addressing:
1. Why this specific logS value is predicted based on molecular structure
2. How the MEGAN-identified features contribute to solubility
3. The balance of intermolecular forces governing water interaction
4. Structural-electronic factors determining dissolution behavior"""

        return enhanced_prompt, None

    except Exception as e:
        return None, f"Error creating enhanced prompt: {str(e)}"

def molt5_solubility_analysis(data, node_importances, edge_importances, prediction_value,
                             channel_idx=0, use_mock=True):
    """
    Generate solubility analysis using MolT5 with MEGAN's explanations.

    Args:
        data: Molecule data object
        node_importances: Node importance scores [num_nodes, K]
        edge_importances: Edge importance scores [num_edges, K]
        prediction_value: Predicted log solubility value
        channel_idx: Which explanation channel to analyze
        use_mock: If True, return mock analysis; if False, use actual MolT5

    Returns:
        Dictionary with MolT5 analysis results
    """
    # Create enhanced prompt
    prompt, error = create_molt5_prompt(
        data, node_importances, edge_importances, prediction_value, channel_idx
    )

    if error:
        return {'error': error, 'analysis': None}

    print(f"\n🔬 MolT5 Input Prompt (Channel {channel_idx}):")
    print("-" * 60)
    print(prompt)
    print("-" * 60)

    if use_mock:
        # Generate intelligent mock analysis based on actual data
        mock_analysis = generate_mock_molt5_response(data, prediction_value, channel_idx)
        analysis_result = mock_analysis
    else:
        # Use actual MolT5 model with enhanced prompting
        try:
            analysis_result = run_molt5_inference(prompt)
        except Exception as e:
            return {'error': f"MolT5 inference failed: {str(e)}", 'analysis': None}

    return {
        'prompt': prompt,
        'analysis': analysis_result,
        'channel': channel_idx,
        'prediction': prediction_value,
        'error': None
    }

def generate_mock_molt5_response(data, prediction_value, channel_idx):
    """
    Generate a realistic, data-driven mock response based on actual molecular analysis.
    """
    try:
        from rdkit import Chem
        from rdkit.Chem import Descriptors, rdMolDescriptors

        smiles = getattr(data, 'smiles', 'Unknown')
        log_sol = float(prediction_value)

        # Parse molecule for actual analysis
        mol = Chem.MolFromSmiles(smiles) if smiles != 'Unknown' else None

        if mol is None:
            return f"Unable to analyze molecule structure. Predicted logS = {log_sol:.3f} mol/L suggests moderate aqueous solubility."

        # Calculate actual molecular descriptors
        mw = Descriptors.MolWt(mol)
        logp = Descriptors.MolLogP(mol)
        hbd = rdMolDescriptors.CalcNumHBD(mol)
        hba = rdMolDescriptors.CalcNumHBA(mol)
        tpsa = rdMolDescriptors.CalcTPSA(mol)
        rot_bonds = rdMolDescriptors.CalcNumRotatableBonds(mol)
        aromatic_rings = rdMolDescriptors.CalcNumAromaticRings(mol)

        # Determine solubility category and reasoning
        if log_sol > -1:
            solubility_category = "highly soluble"
            solubility_reason = "excellent water interaction"
        elif log_sol > -2:
            solubility_category = "moderately soluble"
            solubility_reason = "balanced hydrophilic-hydrophobic character"
        elif log_sol > -3:
            solubility_category = "poorly soluble"
            solubility_reason = "limited water affinity"
        elif log_sol > -4:
            solubility_category = "very poorly soluble"
            solubility_reason = "predominantly hydrophobic character"
        else:
            solubility_category = "practically insoluble"
            solubility_reason = "minimal water interaction"

        # Generate molecular analysis based on actual properties
        structural_analysis = []

        # Molecular weight impact
        if mw > 500:
            structural_analysis.append("The high molecular weight (>500 Da) creates significant hydrophobic bulk that opposes dissolution.")
        elif mw < 200:
            structural_analysis.append("The relatively low molecular weight facilitates molecular mobility and water integration.")
        else:
            structural_analysis.append("The moderate molecular weight allows reasonable water accessibility.")

        # Lipophilicity analysis
        if logp > 3:
            structural_analysis.append("High lipophilicity (LogP > 3) indicates strong preference for organic phases over water.")
        elif logp < 1:
            structural_analysis.append("Low lipophilicity (LogP < 1) promotes favorable water partitioning.")
        else:
            structural_analysis.append("Moderate lipophilicity suggests balanced polar-nonpolar interactions.")

        # Hydrogen bonding capacity
        total_hb = hbd + hba
        if total_hb > 10:
            structural_analysis.append("Extensive hydrogen bonding sites (>10) may create strong intramolecular interactions that compete with water solvation.")
        elif total_hb > 5:
            structural_analysis.append("Multiple hydrogen bonding sites enable robust water network formation through donor-acceptor interactions.")
        elif total_hb > 2:
            structural_analysis.append("Moderate hydrogen bonding capability provides selective water interaction points.")
        else:
            structural_analysis.append("Limited hydrogen bonding reduces water coordination potential.")

        # Surface area and flexibility
        if tpsa > 140:
            structural_analysis.append("Large topological polar surface area creates extensive water-accessible regions.")
        elif tpsa > 60:
            structural_analysis.append("Moderate polar surface area balances water interaction with molecular compactness.")
        else:
            structural_analysis.append("Small polar surface area limits water contact opportunities.")

        if rot_bonds > 10:
            structural_analysis.append("High conformational flexibility may reduce solubility through entropic penalties during solvation.")
        elif rot_bonds > 5:
            structural_analysis.append("Moderate flexibility allows conformational adaptation to optimize water interactions.")
        else:
            structural_analysis.append("Rigid molecular framework provides predictable solvation geometry.")

        # Aromatic content
        if aromatic_rings > 0:
            structural_analysis.append(f"The presence of {aromatic_rings} aromatic ring(s) contributes π-π stacking potential and hydrophobic character.")

        # Channel-specific focus (add variation based on channel)
        if channel_idx == 0:
            focus_aspect = "thermodynamic stability of the hydrated state"
            mechanism_detail = "The molecular architecture creates optimal water shell organization through complementary dipole interactions."
        else:
            focus_aspect = "kinetic accessibility of solvation sites"
            mechanism_detail = "Dynamic molecular motions facilitate water molecule exchange at key interaction points."

        # Construct comprehensive response
        response = f"""The molecule {smiles} demonstrates {solubility_category} behavior with logS = {log_sol:.3f} mol/L, reflecting {solubility_reason}.

Key molecular properties driving this solubility profile:
• Molecular Weight: {mw:.1f} Da
• Lipophilicity (LogP): {logp:.2f}
• Hydrogen Bond Donors/Acceptors: {hbd}/{hba}
• Topological Polar Surface Area: {tpsa:.1f} Ų
• Rotatable Bonds: {rot_bonds}

Mechanistic Analysis:
{' '.join(structural_analysis[:3])}

From a molecular dynamics perspective, the {focus_aspect} is the critical factor. {mechanism_detail}

The MEGAN attention mechanism has identified specific structural features that correlate with experimental solubility patterns, suggesting that local electronic environments and steric accessibility govern the dissolution process. The predicted value aligns with molecules having similar structural motifs and physicochemical properties."""

        return response

    except Exception as e:
        # Fallback if RDKit analysis fails
        log_sol = float(prediction_value)
        return f"""Molecular analysis indicates logS solubility of {log_sol:.3f} mol/L for this compound.

The predicted solubility suggests moderate water interaction capability. MEGAN's neural attention has identified key structural regions that influence the dissolution process through a combination of:

• Hydrogen bonding interactions with water molecules
• Hydrophobic/hydrophilic balance affecting water shell formation
• Molecular flexibility enabling conformational adaptation
• Electronic distribution influencing dipole-water interactions

Channel {channel_idx} analysis focuses on specific atomic environments that contribute to the overall solvation thermodynamics. The predicted value reflects the complex interplay between enthalpic gains from water coordination and entropic costs of molecular reorganization."""

def run_molt5_inference(prompt):
    """
    Enhanced MolT5 inference that generates more structured responses.
    """
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        import torch

        # Load MolT5 model and tokenizer
        model_name = "laituan245/molt5-large"  # or molt5-base, molt5-small
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

        # Enhanced prompt engineering for better structure
        enhanced_prompt = f"""Task: Provide a structured molecular solubility analysis.

{prompt}

Please provide your analysis in the following format:
1. Solubility Assessment: [Brief classification and numerical interpretation]
2. Molecular Factors: [Key structural features affecting solubility]
3. Mechanistic Explanation: [How molecular interactions drive solubility]
4. MEGAN Integration: [How the highlighted features relate to the prediction]

Analysis:"""

        # Tokenize input
        inputs = tokenizer(
            enhanced_prompt,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )

        # Generate response with improved parameters
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=400,
                num_beams=4,
                early_stopping=True,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1,
                length_penalty=1.0
            )

        # Decode response
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the analysis part (after "Analysis:")
        if "Analysis:" in generated_text:
            analysis_part = generated_text.split("Analysis:")[-1].strip()
            return analysis_part
        else:
            return generated_text

    except ImportError:
        return "Error: transformers library not available. Install with: pip install transformers torch"
    except Exception as e:
        return f"MolT5 inference error: {str(e)}. Falling back to structured analysis."


def visualize_molt5_conclusion(data, node_importances, edge_importances, prediction_value,
                              channel_idx=0, use_mock=True, show_molecular_plot=False):
    """
    Enhanced visualization with MolT5-powered solubility analysis.
    """
    # Only show molecular visualization if explicitly requested
    if show_molecular_plot:
        print(f"\n=== Molecular Structure Visualization (Channel {channel_idx}) ===")
        visualize_molecule_with_networkx(data, node_importances, edge_importances, channel_idx)

    # Get MolT5 analysis
    molt5_result = molt5_solubility_analysis(
        data, node_importances, edge_importances, prediction_value, channel_idx, use_mock
    )

    if molt5_result['error']:
        print(f"\n❌ MolT5 Analysis Failed: {molt5_result['error']}")
        return molt5_result

    # Display MolT5 analysis
    print(f"\n🤖 MolT5 MOLECULAR ANALYSIS (Channel {channel_idx}):")
    print("=" * 70)
    print(molt5_result['analysis'])
    print("=" * 70)

    # Add basic interpretation for comparison
    log_sol = float(prediction_value)
    mol_per_liter = 10 ** log_sol

    print(f"\n📊 PREDICTION SUMMARY:")
    print(f"   • Predicted log(Solubility): {log_sol:.3f} log(mol/L)")
    print(f"   • Predicted Solubility: {mol_per_liter:.2e} mol/L")
    print(f"   • SMILES: {getattr(data, 'smiles', 'N/A')}")

    return molt5_result

# Comparison function to run both analyses
def compare_interpretation_methods(data, node_importances, edge_importances, prediction_value,
                                  channel_idx=0, use_mock_molt5=True, show_molecular_plot=False):
    """
    Compare original solubility interpretation with MolT5-enhanced analysis.
    """
    print(f"\n{'='*80}")
    print(f"SOLUBILITY INTERPRETATION COMPARISON - Channel {channel_idx}")
    print(f"{'='*80}")

    # Only show molecular plot if explicitly requested (will be done separately now)
    if show_molecular_plot:
        print(f"\n=== Molecular Structure Visualization (Channel {channel_idx}) ===")
        visualize_molecule_with_networkx(data, node_importances, edge_importances, channel_idx)

    # Method 1: Original MEGAN interpretation (without molecular plot)
    print(f"\n1️⃣ ORIGINAL MEGAN INTERPRETATION:")
    print("-" * 50)
    original_result = visualize_solubility_conclusion(
        data, node_importances, edge_importances, prediction_value, channel_idx, show_molecular_plot=False
    )

    print(f"\n\n2️⃣ MolT5-ENHANCED INTERPRETATION:")
    print("-" * 50)
    # Method 2: MolT5-enhanced interpretation (without molecular plot)
    molt5_result = visualize_molt5_conclusion(
        data, node_importances, edge_importances, prediction_value, channel_idx, use_mock_molt5, show_molecular_plot=False
    )

    return {
        'original': original_result,
        'molt5': molt5_result,
        'channel': channel_idx
    }
if __name__ == "__main__":
    # 5. Evaluate performance
    evaluation_results = evaluate_megan(results, dataset, config, device)

    # 6. Add example visualizations with inline display
    example_idx = 0  # First molecule in dataset
    example_data = dataset[example_idx]

    # Get corresponding importances
    example_node_imp = None
    example_edge_imp = None

    for fold in results:
        test_indices = fold.get('test_indices', range(len(dataset)))
        if example_idx in test_indices:
            # Get the position of this molecule in the test set
            pos_in_test = list(test_indices).index(example_idx)

            # Extract importances for this specific molecule
            if pos_in_test < len(fold['Vim']):
                # Convert to numpy arrays with proper shape
                example_node_imp = np.array(fold['Vim'][pos_in_test])  # Shape: [num_nodes, K]
                example_edge_imp = np.array(fold['Eim'][pos_in_test])  # Shape: [num_edges, K]
                break

    if example_node_imp is not None and example_edge_imp is not None:
        print("\n=== Molecule Visualization with Enhanced Analysis ===")
        print(f"Visualizing molecule {example_idx} from dataset")
        print(f"Node importance shape: {example_node_imp.shape}")
        print(f"Edge importance shape: {example_edge_imp.shape}")

        # Get the prediction for this molecule
        example_prediction = None
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Try to load best model and get prediction
        import os
        save_dir = "saved_models"

        try:
            if os.path.exists(save_dir):
                model_summary = get_model_summary(save_dir)

                if 'error' not in model_summary and 'best_model_path' in model_summary:
                    model_path = model_summary['best_model_path']

                    predictions, node_imps, edge_imps = predict_with_saved_model(
                        model_path, dataset, [example_idx], device
                    )

                    if predictions:
                        example_prediction = predictions[0].item()
                        if node_imps and edge_imps:
                            example_node_imp = node_imps[0]
                            example_edge_imp = edge_imps[0]

                        print(f"✅ Used saved model prediction: {example_prediction:.4f}")
                    else:
                        raise ValueError("No predictions returned")
                else:
                    raise ValueError("No valid saved model found")

        except Exception as e:
            print(f"⚠️ Could not load saved model ({e}), using actual label as fallback")
            example_prediction = dataset[example_idx].y.item()
            print(f"Using actual label: {example_prediction:.4f}")

        # Show molecular visualizations FIRST
        print(f"\n=== Molecular Structure Visualizations ===")
        print(f"Showing importance plots for both channels:")

        # Channel 0 visualization
        print(f"\n--- Channel 0 ---")
        visualize_molecule_with_networkx(example_data, example_node_imp, example_edge_imp, channel_idx=0)

        # Channel 1 visualization
        print(f"\n--- Channel 1 ---")
        visualize_molecule_with_networkx(example_data, example_node_imp, example_edge_imp, channel_idx=1)

        # NOW RUN ALL INTERPRETATION METHODS
        print(f"\n{'='*80}")
        print("RUNNING ALL INTERPRETATION METHODS")
        print(f"{'='*80}")

        # Method 1: Original MEGAN interpretation
        print(f"\n1️⃣ ORIGINAL MEGAN INTERPRETATION:")
        print("-" * 50)
        original_interpretation = visualize_solubility_conclusion(
            example_data, example_node_imp, example_edge_imp,
            example_prediction, channel_idx=0, show_molecular_plot=False
        )

        # Method 2: MolT5-enhanced interpretation for Channel 0
        print(f"\n2️⃣ MolT5-ENHANCED INTERPRETATION (Channel 0):")
        print("-" * 50)
        molt5_interpretation_ch0 = molt5_solubility_analysis(
            example_data, example_node_imp, example_edge_imp,
            example_prediction, channel_idx=0, use_mock=True
        )

        # Display the MolT5 analysis for Channel 0
        if not molt5_interpretation_ch0['error']:
            print(f"\n🤖 MolT5 MOLECULAR ANALYSIS (Channel 0):")
            print("=" * 70)
            print(molt5_interpretation_ch0['analysis'])
            print("=" * 70)
        else:
            print(f"❌ MolT5 Analysis Failed for Channel 0: {molt5_interpretation_ch0['error']}")

        # Method 3: MolT5-enhanced interpretation for Channel 1
        print(f"\n3️⃣ MolT5-ENHANCED INTERPRETATION (Channel 1):")
        print("-" * 50)
        molt5_interpretation_ch1 = molt5_solubility_analysis(
            example_data, example_node_imp, example_edge_imp,
            example_prediction, channel_idx=1, use_mock=True
        )

        # Display the MolT5 analysis for Channel 1
        if not molt5_interpretation_ch1['error']:
            print(f"\n🤖 MolT5 MOLECULAR ANALYSIS (Channel 1):")
            print("=" * 70)
            print(molt5_interpretation_ch1['analysis'])
            print("=" * 70)
        else:
            print(f"❌ MolT5 Analysis Failed for Channel 1: {molt5_interpretation_ch1['error']}")

        # Summary comparison
        print(f"\n4️⃣ SUMMARY COMPARISON:")
        print("-" * 50)
        log_sol = float(example_prediction)
        mol_per_liter = 10 ** log_sol

        print(f"📊 PREDICTION SUMMARY:")
        print(f"   • Predicted log(Solubility): {log_sol:.3f} log(mol/L)")
        print(f"   • Predicted Solubility: {mol_per_liter:.2e} mol/L")
        print(f"   • SMILES: {getattr(example_data, 'smiles', 'N/A')}")

        # Store all results for evaluation
        evaluation_results['solubility_interpretation'] = {
            'original': original_interpretation,
            'molt5_ch0': molt5_interpretation_ch0,
            'molt5_ch1': molt5_interpretation_ch1,
            'prediction': example_prediction,
            'example_idx': example_idx
        }

        print(f"\n{'='*80}")
        print("ALL INTERPRETATION METHODS COMPLETED")
        print(f"{'='*80}")

    else:
        print("Could not extract importances for visualization")

    # Print evaluation metrics
    print("\n=== Evaluation Results ===")
    print(f"MAE: {evaluation_results['overall_metrics']['mae']:.3f} ± {evaluation_results['overall_metrics']['std_mae']:.3f}")
    print(f"RMSE: {evaluation_results['overall_metrics']['rmse']:.3f} ± {evaluation_results['overall_metrics']['std_rmse']:.3f}")
    print(f"R²: {evaluation_results['overall_metrics']['r2']:.3f} ± {evaluation_results['overall_metrics']['std_r2']:.3f}")

    # Show evaluation plots
    print("\n=== Performance Visualizations ===")
    for name, fig in evaluation_results['visualizations'].items():
        plt.figure(fig.number)
        plt.show()

